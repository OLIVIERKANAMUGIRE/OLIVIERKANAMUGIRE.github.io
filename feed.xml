<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://olivierkanamugire.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://olivierkanamugire.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-31T17:39:32+00:00</updated><id>https://olivierkanamugire.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Nonlinear Support Vector Machine (SVM) with Polynomial Kernel</title><link href="https://olivierkanamugire.github.io/blog/2025/svm/" rel="alternate" type="text/html" title="Nonlinear Support Vector Machine (SVM) with Polynomial Kernel"/><published>2025-12-20T00:00:00+00:00</published><updated>2025-12-20T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/svm</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/svm/"><![CDATA[<h2 id="introduction-to-support-vector-machines">Introduction to Support Vector Machines</h2> <p>Support Vector Machines (SVMs) are powerful supervised learning algorithms designed for classification and regression tasks. What makes SVMs special is their unique approach to finding decision boundaries. Instead of trying to fit a probabilistic model to the data, SVMs search for the <strong>optimal separating hyperplane</strong> that creates the largest possible margin between different classes.</p> <p>Think of it this way: if you’re drawing a line to separate two groups of points, you want that line to be as far away as possible from both groups. This “safety margin” helps the model generalize better to new, unseen data.</p> <hr/> <h2 id="the-core-idea-maximum-margin-principle">The Core Idea: Maximum Margin Principle</h2> <p>The fundamental principle behind SVM is <strong>margin maximization</strong>. The margin is the perpendicular distance from the decision boundary (hyperplane) to the nearest data points from each class. These nearest points are called <strong>support vectors</strong> because they literally “support” or define the decision boundary.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/svm_overall-480.webp 480w,/assets/img/svm_overall-800.webp 800w,/assets/img/svm_overall-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/svm_overall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="why-maximize-the-margin">Why Maximize the Margin?</h3> <p>A larger margin provides:</p> <ul> <li><strong>Better generalization</strong>: The model is less sensitive to small variations in the data</li> <li><strong>Robustness</strong>: New data points are less likely to be misclassified</li> <li><strong>Geometric intuition</strong>: The decision is based on clear separation, not just probability</li> </ul> <h3 id="the-mathematical-formulation">The Mathematical Formulation</h3> <p>For a dataset with features <strong>$x$</strong> and binary labels <strong>$y$</strong> (where $y = +1$ or $y = -1$), we want to find a hyperplane defined by weights <strong>$w$</strong> and bias <strong>$b$</strong> such that:</p> \[\min_{\mathbf{w}, b} \; \frac{1}{2}\|\mathbf{w}\|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1\] <p><strong>Breaking this down:</strong></p> <ul> <li> <table> <tbody> <tr> <td>The term $\frac{1}{2}</td> <td>w</td> <td>^2$ measures the inverse of the margin (smaller <strong>w</strong> means larger margin)</td> </tr> </tbody> </table> </li> <li>The constraint $y_i(w^TX_i + b) \ge 1$ ensures all points are correctly classified with at least distance 1 from the boundary</li> <li>Only the support vectors (points closest to the boundary) affect the solution</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/svm_vid.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Illustration of the maximum margin principle. The optimal hyperplane is determined solely by the support vectors (the points touching the margin boundaries), while other points have no influence on the decision boundary. </div> <hr/> <h2 id="handling-real-world-data-soft-margin-svm">Handling Real-World Data: Soft Margin SVM</h2> <p>In practice, data is rarely perfectly separable. Classes often overlap due to noise, outliers, or the inherent nature of the problem. This is where <strong>soft margin SVM</strong> comes in.</p> <h3 id="introducing-slack-variables">Introducing Slack Variables</h3> <p>Soft margin SVM introduces slack variables $(\xi_i)$ that allow some points to violate the margin constraint. The optimization problem becomes:</p> \[\min_{\mathbf{w}, b, \xi} \; \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{m}\xi_i \quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1 - \xi_i, \; \xi_i \ge 0\] <h3 id="the-regularization-parameter-c">The Regularization Parameter C</h3> <p>The parameter <strong>C</strong> controls the trade-off between margin maximization and classification errors:</p> <ul> <li><strong>Large C</strong> (e.g., $C = 1000$): <ul> <li>Penalizes misclassifications heavily</li> <li>Results in narrower margins</li> <li>Lower bias, higher variance</li> <li>Risks overfitting</li> </ul> </li> <li><strong>Small C</strong> (e.g., $C = 0.1$): <ul> <li>Allows more margin violations</li> <li>Results in wider margins</li> <li>Higher bias, lower variance</li> <li>Better generalization</li> </ul> </li> </ul> <p><strong>Practical tip</strong>: Start with $C = 1.0$ and tune using cross-validation.</p> <hr/> <h2 id="the-kernel-trick-unlocking-nonlinear-classification">The Kernel Trick: Unlocking Nonlinear Classification</h2> <p>This is where SVM becomes truly powerful. Many real-world problems cannot be solved with a straight line (or hyperplane). The data might be arranged in circles, spirals, or other complex patterns.</p> <h3 id="what-is-the-kernel-trick">What is the Kernel Trick?</h3> <p>The kernel trick is a mathematical technique that allows us to:</p> <ol> <li>Implicitly map data to a higher-dimensional space</li> <li>Find a linear separator in that high-dimensional space</li> <li><strong>Never actually compute the high-dimensional coordinates</strong></li> </ol> <p>Instead of explicitly transforming each data point $\phi(x)$, we use a kernel function $K(x_1, x_2)$ that computes the inner product in the transformed space directly.</p> <h3 id="common-kernel-functions">Common Kernel Functions</h3> <h4 id="1-linear-kernel">1. Linear Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \mathbf{x}_1^\top \mathbf{x}_2\] <ul> <li>Use case: Data is already linearly separable</li> <li>Fastest to compute</li> </ul> <h4 id="2-polynomial-kernel">2. Polynomial Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = (1 + \mathbf{x}_1^\top \mathbf{x}_2)^d\] <ul> <li>Parameter: degree <strong>d</strong> (typically 2-5)</li> <li>Use case: Data has polynomial decision boundaries</li> <li>Our implementation uses this kernel!</li> </ul> <h4 id="3-radial-basis-function-rbfgaussian-kernel">3. Radial Basis Function (RBF/Gaussian) Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_2\|^2\right)\] <ul> <li>Parameter: $\gamma$ (controls influence radius)</li> <li>Use case: Most versatile, works for many nonlinear patterns</li> <li>Default choice when in doubt</li> </ul> <h4 id="4-sigmoid-kernel">4. Sigmoid Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \tanh(\alpha \mathbf{x}_1^\top \mathbf{x}_2 + c)\] <ul> <li>Use case: Similar to neural network activation</li> <li>Less commonly used</li> </ul> <h3 id="how-the-kernel-trick-works">How the Kernel Trick Works</h3> <p>Consider a simple example with polynomial kernel of degree 2:</p> <ul> <li>Original space: 2D points $(x_1, x_2)$</li> <li>Transformed space: 6D $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$</li> <li>Direct computation: $O(d^2)$ operations</li> <li>Kernel computation: $O(d)$ operations</li> </ul> <p>The kernel computes the same result as the inner product in the transformed space, but much more efficiently!</p> <hr/> <h2 id="when-to-use-svms">When to Use SVMs</h2> <h3 id="svms-excel-when">SVMs Excel When:</h3> <ol> <li><strong>Small to medium-sized datasets</strong> (hundreds to thousands of samples) <ul> <li>Computational cost grows with dataset size</li> <li>Training time is $O(n^2)$ to $O(n^3)$</li> </ul> </li> <li><strong>High-dimensional feature spaces</strong> (many features relative to samples) <ul> <li>SVMs are less prone to overfitting in high dimensions</li> <li>Effective even when features » samples</li> </ul> </li> <li><strong>Clear margin separation exists</strong> <ul> <li>SVMs naturally find the best separating boundary</li> <li>Robust to outliers (with proper C tuning)</li> </ul> </li> <li><strong>Interpretability matters</strong> <ul> <li>Decision boundary is defined by support vectors</li> <li>Can identify which samples are most important</li> </ul> </li> </ol> <h3 id="consider-alternatives-when">Consider Alternatives When:</h3> <ul> <li>Dataset is very large (&gt;100,000 samples) $\to$ Use logistic regression or neural networks</li> <li>Data is very noisy with no clear separation $\to$ Try ensemble methods</li> <li>You need probability estimates $\to$ Use logistic regression or calibrated classifiers</li> <li>Real-time prediction speed is critical $\to$ Simpler models may be faster</li> </ul> <hr/> <h2 id="implementation-from-scratch">Implementation from Scratch</h2> <p>Now let’s implement a nonlinear SVM using Python! We’ll use the <strong>polynomial kernel</strong> and solve the optimization problem using quadratic programming.</p> <h3 id="step-1-the-polynomial-kernel-function">Step 1: The Polynomial Kernel Function</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># libraries
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">cvxopt</span> <span class="kn">import</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">solvers</span>

<span class="c1"># mathematical equation for polynomial kernel
</span><span class="k">def</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">.</span><span class="n">T</span><span class="p">))</span> <span class="o">**</span> <span class="n">degree</span>
</code></pre></div></div> <p><strong>Why add the +1?</strong> The constant term ensures that the kernel includes lower-degree polynomial terms as well. For example, with degree=2, we get terms involving $x_1^2, x_2^2, x_1x_2, x_1, x_2$, and the constant.</p> <h3 id="step-2-training-the-nonlinear-svm">Step 2: Training the Nonlinear SVM</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Train a nonlinear SVM with a polynomial kernel using Quadratic Programming.
    </span><span class="sh">"""</span>

    <span class="c1"># Convert labels from {1, 2} to {+1, -1} for mathematical convenience
</span>    <span class="n">trainclass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">#  Compute the Kernel Matrix:  K[i,j] represents the kernel similarity between sample i and sample j
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">traindata</span><span class="p">)</span>

    <span class="c1"># Set up the Quadratic Programming problem
</span>    <span class="c1"># We're solving: min (1/2)α^T P α + q^T α  subject to: Gα ≤ h and Aα = b
</span>    <span class="c1"># P matrix: (y_i * y_j) * K(x_i, x_j)
</span>    <span class="n">P</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">trainclass</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># q vector: all -1s (from the dual formulation)
</span>    <span class="n">q</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

    <span class="c1"># G and h: inequality constraints (α_i ≥ 0). We use -I and 0 to represent α_i ≥ 0
</span>    <span class="n">G</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

    <span class="c1"># A and b: equality constraint (Σ α_i * y_i = 0)
</span>    <span class="n">A</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">trainclass</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="c1">#  Solve the QP problem, we suppress solver output when option is False.
</span>    <span class="n">solvers</span><span class="p">.</span><span class="n">options</span><span class="p">[</span><span class="sh">'</span><span class="s">show_progress</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">solvers</span><span class="p">.</span><span class="nf">qp</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># Extract Lagrange multipliers
</span>    <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">])</span>

    <span class="c1"># Identify support vectors: Support vectors have $\alpha &gt; 0$ (choose a small threshold for numerical stability)
</span>    <span class="n">sv_idx</span> <span class="o">=</span> <span class="n">alphas</span> <span class="o">&gt;</span> <span class="mf">1e-5</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>
    <span class="n">svs</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>
    <span class="n">sv_classes</span> <span class="o">=</span> <span class="n">trainclass</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>

    <span class="c1">#  Compute the bias term w0: we use the KKT conditions: for support vectors on the margin,
</span>    <span class="c1"># y_i * (Σ α_j * y_j * K(x_i, x_j) + w0) = 1
</span>    <span class="c1"># Therefore: w0 = y_i - Σ α_j * y_j * K(x_i, x_j)
</span>    <span class="c1"># We average over all support vectors for numerical stability
</span>    <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">([</span>
        <span class="n">sv_classes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span> <span class="o">*</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">svs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">svs</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">))</span>
    <span class="p">])</span>

    <span class="k">return</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span>
</code></pre></div></div> <p><strong>Key concepts in the code:</strong></p> <ol> <li><strong>Quadratic Programming</strong>: SVM training is formulated as a QP problem in the dual form</li> <li><strong>Support Vectors</strong>: Only points with α &gt; 0 actually contribute to the decision boundary</li> <li><strong>Kernel Matrix</strong>: All pairwise similarities are precomputed for efficient optimization</li> <li><strong>Bias Computation</strong>: Averaged over support vectors for better numerical stability</li> </ol> <h3 id="step-3-making-predictions">Step 3: Making Predictions</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_svm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Make predictions using the trained SVM.

    inputs:

    X : ndarray of shape (n_samples, n_features) / validationo set array
    svs : ndarray - Support vectors from training
    sv_classes : ndarray - Labels of support vectors
    alphas : ndarray -  Lagrange multipliers
    w0 : float -   Bias term

    output:

    predictions : ndarray - predicted class labels (+1 or -1)
    decision_scores : ndarray - raw decision function values (distance from boundary)
    </span><span class="sh">"""</span>
    <span class="c1"># Compute kernel between test points and support vectors
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">svs</span><span class="p">)</span>

    <span class="c1"># Decision function: f(x) = Σ α_i * y_i * K(x, x_i) + w0
</span>    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span><span class="p">)</span> <span class="o">+</span> <span class="n">w0</span>

    <span class="c1"># Predictions: sign of decision function
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">decision_scores</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">decision_scores</span>
</code></pre></div></div> <h3 id="step-4-visualizing-the-decision-boundary">Step 4: Visualizing the Decision Boundary</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the decision boundary, margins, and support vectors.

    This visualization helps understand:
    - How the SVM separates the classes
    - Which points are support vectors
    - The shape of the nonlinear decision boundary
    </span><span class="sh">"""</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># Plot the training data
</span>    <span class="c1"># Class 1 in blue, Class 2 in red
</span>    <span class="n">class1_mask</span> <span class="o">=</span> <span class="n">trainclass</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">class2_mask</span> <span class="o">=</span> <span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="n">class1_mask</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traindata</span><span class="p">[</span><span class="n">class1_mask</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Class 1</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="n">class2_mask</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traindata</span><span class="p">[</span><span class="n">class2_mask</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Class 2</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="c1"># Highlight support vectors with black circles
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">svs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">svs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">facecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Support Vectors</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Create a fine grid to evaluate the decision function
</span>    <span class="n">xlim</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">()</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">))</span>

    <span class="c1"># Evaluate decision function on every point in the grid
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()]</span>
    <span class="n">K_grid</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">svs</span><span class="p">)</span>
    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K_grid</span><span class="p">,</span> <span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span><span class="p">)</span> <span class="o">+</span> <span class="n">w0</span>
    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">decision_scores</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Plot the decision boundary (where decision_score = 0)
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">'</span><span class="s">solid</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Plot the margins (where decision_score = ±1)
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">'</span><span class="s">dashed</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="c1"># Fill the decision regions with light colors
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">],</span>
                 <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">lightblue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">lightcoral</span><span class="sh">'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Nonlinear SVM with Polynomial Kernel</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="step-5-putting-it-all-together">Step 5: Putting It All Together</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Load your data (replace with actual data loading)
</span>    <span class="c1"># Example of data set ono my github:
</span>    <span class="c1"># traindata = np.loadtxt('t030.csv', delimiter=',', usecols=(0,1))
</span>    <span class="c1"># trainclass = np.loadtxt('t030.csv', delimiter=',', usecols=(2,))
</span>
    <span class="c1">#Famous Example of cirular rings to show importance of kernel trick
</span>
    <span class="c1"># reproducibility
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Generate circular data
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">radius1</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">radius2</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="c1"># Inner circle (class 1)
</span>    <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">radius1</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">([</span><span class="n">r1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta1</span><span class="p">),</span> <span class="n">r1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta1</span><span class="p">)])</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Outer ring (class 2)
</span>    <span class="n">theta2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">radius1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">radius2</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">([</span><span class="n">r2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta2</span><span class="p">),</span> <span class="n">r2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta2</span><span class="p">)])</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">traindata</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
    <span class="n">trainclass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">])</span>

    <span class="c1"># Train the SVM
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Nonlinear SVM...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of support vectors: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span><span class="si">}</span><span class="s"> out of </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span><span class="si">}</span><span class="s"> training samples</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Bias term (w0): </span><span class="si">{</span><span class="n">w0</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Visualize
</span>    <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>

    <span class="c1"># Make predictions on training data
</span>    <span class="n">predictions</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="nf">predict_svm</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training accuracy: </span><span class="si">{</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <p>Results from that randomly generated samples with rings of points: famous example showing importance of kernel trick</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/svm_image-480.webp 480w,/assets/img/svm_image-800.webp 800w,/assets/img/svm_image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/svm_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="understanding-the-output">Understanding the Output</h2> <p>When you run this code, you’ll see:</p> <ol> <li> <p><strong>Support Vectors</strong>: Typically only a small fraction of training points become support vectors. These are the “critical” points that define the boundary.</p> </li> <li> <p><strong>Decision Boundary</strong>: The black solid line shows where the classifier switches between classes (decision_score = 0).</p> </li> <li> <p><strong>Margins</strong>: The dashed lines show the margin boundaries (decision_score = $\pm 1$). Support vectors lie on or within these margins.</p> </li> <li> <p><strong>Decision Regions</strong>: The shaded areas show which class the SVM predicts for each region of space.</p> </li> </ol> <hr/> <h2 id="hyperparameter-tuning-tips">Hyperparameter Tuning Tips</h2> <h3 id="comparative-analysis-choosing-c-regularization-parameter">Comparative analysis: Choosing C (Regularization Parameter)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Try different C values
</span><span class="n">C_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>
    <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">C=</span><span class="si">{</span><span class="n">C</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span><span class="si">}</span><span class="s"> support vectors</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><strong>Small C</strong> $(0.01-0.1)$: More support vectors, wider margin, better generalization</li> <li><strong>Large C</strong> $(10-100)$: Fewer support vectors, narrower margin, risk of overfitting</li> </ul> <h3 id="choosing-polynomial-degree">Choosing Polynomial Degree</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Modify the kernel function to accept different degrees
</span><span class="k">def</span> <span class="nf">svm_with_degree</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
    <span class="c1"># Modify polynomial_kernel calls to use specified degree
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">traindata</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1">#  training codes ...
</span></code></pre></div></div> <ul> <li><strong>degree=2</strong>: Good starting point for most problems</li> <li><strong>degree=3-4</strong>: More complex boundaries, risk of overfitting</li> <li><strong>degree&gt;5</strong>: Rarely useful, very high computational cost</li> </ul> <hr/> <h2 id="common-pitfalls-and-solutions">Common Pitfalls and Solutions</h2> <h3 id="1-numerical-instability">1. Numerical Instability</h3> <p><strong>Problem</strong>: Kernel values become too large or too small <strong>Solution</strong>: Feature normalization</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">traindata</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span>
</code></pre></div></div> <h3 id="2-all-points-become-support-vectors">2. All Points Become Support Vectors</h3> <p><strong>Problem</strong>: C is too large or data has no clear separation <strong>Solution</strong>: Reduce C or try a different kernel</p> <h3 id="3-poor-performance">3. Poor Performance</h3> <p><strong>Problem</strong>: Wrong kernel choice or bad hyperparameters <strong>Solution</strong>: Use cross-validation to tune C and kernel parameters</p> <h3 id="4-slow-training">4. Slow Training</h3> <p><strong>Problem</strong>: Dataset is too large <strong>Solution</strong>: Consider using scikit-learn’s SVC with better optimization or subsample the data</p> <hr/> <h2 id="extending-this-implementation">Extending This Implementation</h2> <p>Want to take this further? Try:</p> <ol> <li><strong>Implement RBF kernel</strong>: Replace polynomial with Gaussian kernel</li> <li><strong>Add cross-validation</strong>: Use k-fold CV to select optimal C</li> <li><strong>Multi-class classification</strong>: Extend to handle more than two classes using one-vs-rest or one-vs-one</li> <li><strong>Feature importance</strong>: Analyze which support vectors contribute most</li> <li><strong>Compare with sklearn</strong>: Validate your implementation against scikit-learn</li> </ol> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Support Vector Machines provide an elegant combination of:</p> <ul> <li><strong>Geometric intuition</strong> (maximum margin principle)</li> <li><strong>Mathematical rigor</strong> (convex optimization)</li> <li><strong>Practical power</strong> (kernel trick for nonlinearity)</li> </ul> <p>By implementing SVM from scratch, we gain deep insights into:</p> <ul> <li>How the optimization problem is formulated</li> <li>Why support vectors are special</li> <li>How kernels enable nonlinear classification without explicit feature transformation</li> </ul> <p>While libraries like scikit-learn offer highly optimized implementations, understanding the fundamentals helps you make better choices about when and how to apply SVMs in practice.</p> <hr/> <h2 id="references-and-further-reading">References and Further Reading</h2> <ul> <li><strong>Original SVM Papers</strong>: Cortes, C., &amp; Vapnik, V. (1995) “Support-vector networks” Machine Learning, 20(3), 273-297. [The original paper introducing Support Vector Machines]</li> <li><strong>Kernel Methods</strong>: Schölkopf, B., &amp; Smola, A. J. (2002) “Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond” MIT Press, Cambridge, MA.</li> <li><strong>Practical Guide</strong>: Hsu, C. W., Chang, C. C., &amp; Lin, C. J. (2003) “A Practical Guide to Support Vector Classification” Technical Report, Department of Computer Science, National Taiwan University. Available: https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf [Essential practical guide for parameter selection and implementation]</li> <li><strong>Bayesian perceptive</strong>: “Pattern Recognition and Machine Learning” Springer, New York. Chapter 7: Sparse Kernel Machines [Modern treatment with Bayesian perspective] Cristianini, N., &amp; Shawe-Taylor, J. (2000)</li> <li><strong>Implementation</strong>: <a href="scikit-learn SVM documentation">https://scikit-learn.org/stable/modules/svm.html</a></li> </ul> <hr/> <h2 id="code-repository">Code Repository</h2> <p>The complete implementation with example datasets is available on <a href="GitHub -SVM">https://github.com/OLIVIERKANAMUGIRE/Support-Vector-Machines-from-Scratch</a></p> <p>Dataset used: <code class="language-plaintext highlighter-rouge">t030.csv</code> - A small nonlinearly separable 2D dataset for demonstration.</p> <hr/> <p><em>Happy learning! If you have questions or suggestions, feel free to reach out or open an issue on GitHub.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Nonlinear support vector machine from scratch]]></summary></entry><entry><title type="html">Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction</title><link href="https://olivierkanamugire.github.io/blog/2025/svdappli/" rel="alternate" type="text/html" title="Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction"/><published>2025-01-12T00:00:00+00:00</published><updated>2025-01-12T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/svdappli</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/svdappli/"><![CDATA[<h2 id="introduction-and-objective">Introduction and Objective</h2> <p>Hyperspectral images contain rich spectral information across hundreds of wavelength bands, making them highly informative but also computationally expensive to store and process. Dimensionality reduction techniques are therefore essential for efficient analysis and visualization.</p> <p>In this work, <strong>Singular Value Decomposition (SVD)</strong> is applied to the well-known <strong>Indian Pines hyperspectral dataset</strong> to explore its structure and reduce its dimensionality while preserving most of the informative content. The main objective is to analyze the dataset, perform dimensionality reduction, and evaluate reconstruction quality.</p> <hr/> <h2 id="problem-statement">Problem Statement</h2> <p>Given the MATLAB file <code class="language-plaintext highlighter-rouge">indian_pines.mat</code>, the following tasks are performed:</p> <ol> <li>Load the hyperspectral data and visualize it in RGB format</li> <li>Generate a false-color image and analyze the spectral profile of a selected pixel</li> <li>Compute the total number of elements required to store the dataset</li> <li>Reshape the 3D hyperspectral cube into a 2D matrix</li> <li>Apply Singular Value Decomposition (SVD)</li> <li>Analyze cumulative explained variance and select an appropriate reconstruction rank</li> <li>Reconstruct the dataset using truncated SVD</li> <li>Compute dimensionality reduction percentage and reconstruction error (RMSE)</li> </ol> <hr/> <h2 id="methodology">Methodology</h2> <p>All computations and visualizations were performed using <strong>MATLAB</strong> [check corresponding github].</p> <h3 id="loading-and-visualization">Loading and Visualization</h3> <p>The hyperspectral image cube is loaded and visualized in RGB format. A false-color image is created by selecting specific spectral bands and applying contrast stretching. Pixel-level spectral behavior is analyzed, and the 3D data cube is reshaped into a 2D matrix using MATLAB’s built-in <code class="language-plaintext highlighter-rouge">reshape()</code> function.</p> <hr/> <h3 id="singular-value-decomposition">Singular Value Decomposition</h3> <p>After reshaping, Singular Value Decomposition is applied:</p> \[\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{V}^\top\] <p>The cumulative explained variance (CV) is computed as:</p> \[\text{CV} = \text{cumsum}(EV), \quad \text{where} \quad EV = \frac{\sigma_i^2}{\sum_i \sigma_i^2}\] <p>Here, $\sigma_i$ represents the singular values contained in the diagonal matrix $\mathbf{S}$. The reconstruction rank is selected based on the desired variance threshold.</p> <hr/> <h3 id="dimensionality-and-error-analysis">Dimensionality and Error Analysis</h3> <p>Using truncated SVD components, the dataset is reconstructed. The reduced dimensionality is computed as:</p> \[\text{Dim}_{\text{reduced}} = \text{elements}(U_k S_k) + \text{elements}(V_k)\] <p>The percentage reduction in dimensionality and the <strong>Root Mean Squared Error (RMSE)</strong> between the original and reconstructed datasets are used to quantify performance.</p> <hr/> <h2 id="results">Results</h2> <h3 id="image-loading-properties-and-visualization">Image Loading, Properties, and Visualization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img1-480.webp 480w,/assets/img/ADML/img1-800.webp 800w,/assets/img/ADML/img1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img2-480.webp 480w,/assets/img/ADML/img2-800.webp 800w,/assets/img/ADML/img2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The left image shows the RGB visualization, while the right image presents a false-color representation with contrast stretching.</p> <p><strong>Dataset properties:</strong></p> <p>Hypercube: DataCube: [145 × 145 × 220 double] Wavelength: [220 × 1 double]</p> <hr/> <h3 id="spectral-profile-of-a-pixel">Spectral Profile of a Pixel</h3> <p>A pixel located at <strong>row 60, column 70</strong> was selected to analyze its spectral behavior.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img3-480.webp 480w,/assets/img/ADML/img3-800.webp 800w,/assets/img/ADML/img3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The spectral profile indicates a significantly higher intensity around wavelength index 50 for this pixel.</p> <hr/> <h3 id="data-reshaping-and-storage-analysis">Data Reshaping and Storage Analysis</h3> <p>The total number of elements required to store the hyperspectral cube is:</p> \[145 \times 145 \times 220 = 4, 625, 500\] <p>The 3D matrix is reshaped into a 2D matrix of size:</p> \[(145 \times 145) \times 220 = 21 025 \times 220\] <hr/> <h3 id="svd-dimensionality-reduction-and-error-analysis">SVD, Dimensionality Reduction, and Error Analysis</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img5-480.webp 480w,/assets/img/ADML/img5-800.webp 800w,/assets/img/ADML/img5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The cumulative explained variance increases rapidly, indicating that a <strong>small number of singular values capture most of the information</strong>. Remarkably, <strong>99% of the variance is captured with rank = 2</strong>, resulting in a dimensionality reduction of approximately <strong>96.35%</strong>.</p> <hr/> <h3 id="reconstructed-image">Reconstructed Image</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img4-480.webp 480w,/assets/img/ADML/img4-800.webp 800w,/assets/img/ADML/img4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The reconstructed image retains nearly all structural information while using only a fraction of the original data dimensions.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>This experiment highlights the effectiveness of Singular Value Decomposition for hyperspectral image analysis. Despite the high dimensionality of the Indian Pines dataset, only a very low-rank approximation is required to preserve most of the information.</p> <p>Key observations include:</p> <ul> <li>Rapid growth of cumulative explained variance</li> <li>Significant reduction in storage requirements</li> <li>High-quality reconstruction with minimal loss</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Singular Value Decomposition proves to be a powerful and interpretable tool for dimensionality reduction in hyperspectral imaging. Its ability to compress data while preserving essential information makes it suitable for preprocessing, visualization, and further machine learning tasks.</p> <p>Future extensions may include:</p> <ul> <li>PCA comparison</li> <li>noise robustness analysis</li> <li>band selection strategies</li> <li>integration with classification models</li> </ul> <hr/> <p><em>Code and figures related to this analysis are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="linear-algebra"/><category term="svd"/><category term="hyperspectral"/><category term="dimensionality-reduction"/><summary type="html"><![CDATA[Dimensionality reduction of the Indian Pines hyperspectral dataset using Singular Value Decomposition (SVD)]]></summary></entry><entry><title type="html">Logistic Regression for Ultrasonic Flow Meter Health Classification</title><link href="https://olivierkanamugire.github.io/blog/2025/logistic_regression/" rel="alternate" type="text/html" title="Logistic Regression for Ultrasonic Flow Meter Health Classification"/><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/logistic_regression</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/logistic_regression/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Condition monitoring and diagnostics play a critical role in ensuring the reliability of industrial measurement systems. In ultrasonic flow metering, installation effects can significantly distort measurements, leading to inaccurate flow estimation and potential operational risks.</p> <p>This work focuses on developing a <strong>binary classification model based on logistic regression</strong> to determine whether an 8-path liquid ultrasonic flow meter (USM) is operating in a <strong>healthy state</strong> or is affected by <strong>installation effects</strong>. Using diagnostic parameters derived from experimental observations, we formulate the problem as a supervised learning task and analyze both optimization behavior and classification performance.</p> <hr/> <h2 id="problem-formulation">Problem Formulation</h2> <p>Given a dataset<br/> \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^{N},\) where $x_i \in \mathbb{R}^{36}$ represents diagnostic features and</p> \[y_i \in \{0,1\}\] <p>denotes the health state of the flow meter, the objective is to learn a parameter vector $\beta$ that models the probability</p> \[P(y = 1 \mid x) = \sigma(x^\top \beta),\] <p>where $\sigma(\cdot)$ is the sigmoid function.</p> <hr/> <h2 id="about-the-dataset">About the Dataset</h2> <p>The dataset, referred to as <strong>Meter A</strong>, consists of 87 experimental observations collected from an 8-path liquid ultrasonic flow meter .</p> <p><strong>Key characteristics:</strong></p> <ul> <li>87 total samples</li> <li>36 continuous input features</li> <li>Binary target variable: <ul> <li><code class="language-plaintext highlighter-rouge">1</code> → Healthy condition</li> <li><code class="language-plaintext highlighter-rouge">0</code> → Installation effect</li> </ul> </li> </ul> <p>The relatively small dataset size motivated careful preprocessing and evaluation to avoid biased conclusions.</p> <hr/> <h2 id="data-preprocessing">Data Preprocessing</h2> <h3 id="feature-normalization">Feature Normalization</h3> <p>Initial exploration revealed that the predictors varied significantly in scale and dispersion. To ensure numerical stability during optimization, all features were standardized using <strong>Z-score normalization</strong>:</p> \[z = \frac{x - \mu}{\sigma},\] <p>where $\mu$ and $\sigma$ denote the mean and standard deviation of each feature.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/box1-480.webp 480w,/assets/img/ADML/box1-800.webp 800w,/assets/img/ADML/box1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/box1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/box2-480.webp 480w,/assets/img/ADML/box2-800.webp 800w,/assets/img/ADML/box2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/box2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>No missing values or categorical variables were detected in the dataset.</p> <hr/> <h2 id="traintest-splitting-strategy">Train–Test Splitting Strategy</h2> <p>Due to the sequential ordering of healthy and faulty observations, the dataset was <strong>shuffled prior to splitting</strong> to ensure representative samples in both subsets.</p> <p>Multiple split ratios were evaluated: 60:40, 75:25, 80:20.</p> <p>This allowed us to analyze how training data size affects convergence, generalization, and classification accuracy.</p> <hr/> <h2 id="logistic-regression-model">Logistic Regression Model</h2> <h3 id="sigmoid-function">Sigmoid Function</h3> <p>Logistic regression maps linear combinations of features to probabilities using the sigmoid function:</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}.\] <p>This formulation enables direct probabilistic interpretation of model outputs.</p> <hr/> <h2 id="cost-function-and-optimization">Cost Function and Optimization</h2> <p>The model parameters were estimated by minimizing the <strong>negative log-likelihood</strong>:</p> \[\mathcal{L}(\beta) = - \sum_{i=1}^{n} \left[ y_i \log(\sigma(X_i \beta)) + (1 - y_i)\log(1 - \sigma(X_i \beta)) \right].\] <h3 id="optimization-method">Optimization Method</h3> <ul> <li>Initial weights: <strong>zero initialization</strong></li> <li>Bias term embedded directly into the feature matrix</li> <li>Optimization performed using <strong>MATLAB’s <code class="language-plaintext highlighter-rouge">fminsearch</code></strong></li> <li>No explicit stopping criteria imposed</li> </ul> <p>A custom output function tracked:</p> <ul> <li>loss value per iteration</li> <li>total number of iterations until convergence</li> </ul> <p>This setup allowed us to study the <strong>natural convergence behavior</strong> of the optimizer.</p> <hr/> <h2 id="optimization-behavior">Optimization Behavior</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch60-480.webp 480w,/assets/img/ADML/fminsearch60-800.webp 800w,/assets/img/ADML/fminsearch60-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch60.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch75-480.webp 480w,/assets/img/ADML/fminsearch75-800.webp 800w,/assets/img/ADML/fminsearch75-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch75.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch80-480.webp 480w,/assets/img/ADML/fminsearch80-800.webp 800w,/assets/img/ADML/fminsearch80-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch80.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Key observations:</strong></p> <ul> <li>All splits show monotonic decrease in loss</li> <li>Larger training sets require more optimization iterations</li> <li>Absence of stopping criteria increases computation cost for larger datasets</li> </ul> <hr/> <h2 id="model-evaluation-metrics">Model Evaluation Metrics</h2> <p>Model performance was assessed using:</p> <ul> <li><strong>Accuracy</strong></li> <li><strong>Confusion Matrix</strong></li> <li><strong>Precision</strong></li> <li><strong>Recall</strong></li> </ul> \[\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}\] <hr/> <h2 id="classification-accuracy-results">Classification Accuracy Results</h2> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">pretty_table</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| split ratio | Tr. accuracy | Te. accuuracy |
| :---------- | :----------: | ------------: |
| 60:40       |    80.86     |         82.86 |
| 75:25       |    83.08     |        100.00 |
| 80:20       |     90.0     |         76.47 |
</code></pre></div></div> <p>The <strong>75:25 split</strong> achieved perfect test accuracy, though this should be interpreted cautiously due to the small sample size.</p> <hr/> <h2 id="feature-importance-analysis">Feature Importance Analysis</h2> <p>Across all splits, the <strong>25th feature</strong> (gain at the fifth end of each of the eight paths) consistently exhibited the largest coefficient magnitude.</p> <ul> <li>Strong influence regardless of split ratio</li> <li>Indicates a dominant diagnostic role in identifying installation effects</li> </ul> <hr/> <h2 id="confusion-matrix-analysis">Confusion Matrix Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf60-480.webp 480w,/assets/img/ADML/conf60-800.webp 800w,/assets/img/ADML/conf60-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf60.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf75-480.webp 480w,/assets/img/ADML/conf75-800.webp 800w,/assets/img/ADML/conf75-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf75.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf80-480.webp 480w,/assets/img/ADML/conf80-800.webp 800w,/assets/img/ADML/conf80-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf80.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="discussion">Discussion</h2> <p>This study demonstrates that logistic regression, when combined with appropriate normalization and numerical optimization, can effectively classify ultrasonic flow meter health states—even with limited data.</p> <p>However:</p> <ul> <li>Small datasets increase variance in performance metrics</li> <li>Absence of stopping criteria impacts computational efficiency</li> <li>Feature dominance suggests opportunities for dimensionality reduction</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Logistic regression remains a powerful and interpretable baseline for industrial diagnostic problems. Beyond classification accuracy, analyzing <strong>optimization dynamics</strong>, <strong>feature influence</strong>, and <strong>data splitting strategies</strong> provides deeper insight into model behavior and reliability.</p> <p>Future work may include:</p> <ul> <li>regularization</li> <li>alternative optimizers</li> <li>cross-validation</li> <li>nonlinear extensions</li> </ul> <hr/> <p><em>Code and experiments related to this project are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="machine-learning"/><category term="classification"/><category term="optimization"/><summary type="html"><![CDATA[Binary classification of ultrasonic flow meter health using logistic regression and numerical optimization]]></summary></entry><entry><title type="html">On the optimization algorithms and learning rate</title><link href="https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm/" rel="alternate" type="text/html" title="On the optimization algorithms and learning rate"/><published>2025-01-05T00:00:00+00:00</published><updated>2025-01-05T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Optimization lies at the heart of machine learning. Nearly every learning problem can be framed as the task of minimizing (or maximizing) an objective function that quantifies error, loss, or energy. Whether we are fitting a linear model or training a deep neural network with millions of parameters, the learning process reduces to an optimization problem.</p> <p>This article introduces the fundamental ideas behind optimization in machine learning and gradually builds intuition for some of the most widely used algorithms: <strong>Gradient Descent (GD)</strong>, <strong>Stochastic Gradient Descent (SGD)</strong>, and <strong>Adam</strong>.</p> <hr/> <h2 id="optimization-as-a-minimization-problem">Optimization as a Minimization Problem</h2> <p>In supervised learning, we are given a dataset $\mathcal{D} = {(x_i, y_i)}_{i=1}^N$and a model parameterized by $\theta$. Learning consists of minimizing a loss function:</p> \[\theta^* = \arg\min_{\theta} \mathcal{L}(\theta), \quad \text{where} \quad \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell\big(f(x_i; \theta), y_i\big).\] <p>The loss surface can be convex or non-convex, smooth or highly irregular, low- or high-dimensional.</p> <p>Optimization algorithms define <em>how</em> we move on this surface to reach a good solution.</p> <hr/> <h2 id="gradient-descent-gd">Gradient Descent (GD)</h2> <p>Gradient Descent is the most fundamental first-order optimization method. It iteratively updates parameters in the direction of the negative gradient of the loss function:</p> \[\theta_{t+1} = \theta_t - \eta \nabla_{\theta}\mathcal{L}(\theta_t)\] <p>where $\nabla_{\theta}\mathcal{L}$ is the gradient and $\eta &gt; 0$ is the learning rate.</p> <h2 id="example-of-its-convergence">Example of its convergence</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/gradient_descent_convergence.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="intuition">Intuition</h3> <p>The gradient points in the direction of steepest ascent. Moving in the opposite direction ensures we descend toward a minimum. However, the choice of learning rate is critical.</p> <p>The animations here illustrate this clearly:</p> <ul> <li><strong>Small learning rate</strong>: stable but slow convergence</li> <li><strong>Large learning rate</strong>: faster movement but risk of overshooting or divergence</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/small_lr.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/large_lr.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <h3 id="limitations">Limitations</h3> <ul> <li>Requires computing gradients over the <em>entire dataset</em></li> <li>Slow for large-scale problems</li> <li>Sensitive to learning rate selection</li> </ul> <hr/> <h2 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h2> <p>To address scalability, <strong>Stochastic Gradient Descent</strong> approximates the full gradient using a single sample or a mini-batch:</p> \[\theta_{t+1} = \theta_t - \eta \nabla_{\theta}\ell(f(x_i; \theta_t), y_i)\] <h3 id="why-sgd-works">Why SGD Works</h3> <p>Although SGD introduces noise into the optimization process, this randomness often helps:</p> <ul> <li>escape shallow local minima</li> <li>improve generalization</li> <li>speed up training dramatically</li> </ul> <h3 id="trade-offs">Trade-offs</h3> <ul> <li>Faster per-iteration updates</li> <li>Noisy convergence</li> <li>Requires careful tuning of learning rate schedules</li> </ul> <hr/> <h2 id="momentum-based-methods">Momentum-Based Methods</h2> <p>Pure SGD may oscillate heavily, especially in ravines where curvature differs across dimensions. Momentum methods address this by accumulating past gradients:</p> <p>$v_t = \beta v_{t-1} + (1 - \beta)\nabla_{\theta}\mathcal{L}(\theta_t)$ $\theta_{t+1} = \theta_t - \eta v_t$</p> <p>Momentum smooths updates and accelerates convergence along consistent directions.</p> <hr/> <h2 id="adam-optimizer">Adam Optimizer</h2> <p><strong>Adam (Adaptive Moment Estimation)</strong> combines:</p> <ul> <li>momentum (first moment)</li> <li>adaptive learning rates (second moment)</li> </ul> <p>It maintains running estimates of:</p> <ul> <li>mean of gradients</li> <li>uncentered variance of gradients</li> </ul> <p>Update rules:</p> <p>\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\) \(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\)</p> <p>After bias correction:</p> \[\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\] <h2 id="example-of-its-convergence-1">Example of its convergence</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/adam_convergence.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="why-adam-is-popular">Why Adam is Popular</h3> <ul> <li>Minimal tuning</li> <li>Handles sparse gradients well</li> <li>Works robustly in deep learning</li> </ul> <p>However, Adam is not always optimal for generalization and may converge to worse minima than SGD in some settings.</p> <hr/> <h2 id="choosing-the-right-optimizer">Choosing the Right Optimizer</h2> <p>There is no universally optimal optimizer. The choice depends on:</p> <ul> <li>dataset size</li> <li>model complexity</li> <li>smoothness of the loss</li> <li>generalization requirements</li> </ul> <p><strong>Rule of thumb</strong>:</p> <ul> <li>Start with <strong>Adam</strong> for fast prototyping</li> <li>Switch to <strong>SGD + momentum</strong> for better generalization when training stabilizes</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Optimization algorithms define the dynamics of learning. Understanding how and why they work is essential not only for training models efficiently, but also for diagnosing failure modes such as divergence, slow convergence, or poor generalization.</p> <p>From classical Gradient Descent to adaptive methods like Adam, optimization remains an active and evolving research area—especially in large-scale and multimodal learning systems.</p> <hr/> <p><em>Code implementations and experiments related to these algorithms are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[Gradient descent (GD), Stochastic GD, Adam optimizer]]></summary></entry><entry><title type="html">Graphs and Graph Neural Nets</title><link href="https://olivierkanamugire.github.io/blog/2024/graphNN/" rel="alternate" type="text/html" title="Graphs and Graph Neural Nets"/><published>2024-12-20T00:00:00+00:00</published><updated>2024-12-20T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2024/graphNN</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2024/graphNN/"><![CDATA[<h2 id="graph-neural-nets-and-graph-convolutional-nets">Graph Neural Nets and Graph Convolutional Nets</h2> <p>Graphs are very rich mathematical-defined data structures. Research on analysing graphs with machine learning has been receiving more and more attention because of the great expressive power of graphs. Some real-world problems do not fit into sequential data-related or grid structures like images. Due to graph structure, it enables relational reasoning and efficient representation learning for entities connected in a network. They excel in problems involving relationships between entities. Another thing is that it captures a variety of settings: supervised, semi-supervised and unsupervised.</p> <hr/> <h2 id="problems-that-gnns-and-gcns-are-particularly-good-at-addressing">Problems That GNNs and GCNs Are Particularly Good at Addressing:</h2> <p><strong>Node Classification</strong> – Determining the labels of nodes by analyzing their features and neighbouring nodes (e.g., identifying fraud in financial transactions).</p> <p><strong>Link Prediction</strong> – Anticipating absent or upcoming connections among nodes (e.g., suggesting friends in social networks).</p> <p><strong>Graph Classification</strong> – Categorizing whole graphs (e.g., assessing whether a chemical compound is harmful or safe).</p> <p><strong>Clustering and Community Detection</strong> – Recognizing clusters within networks (e.g., identifying similar consumers in e-commerce).</p> <p><strong>Knowledge Graph Completion</strong> – Deducing absent relationships between entities within a knowledge base (e.g., systems for answering questions).</p> <h2 id="applications">Applications</h2> <p><strong>Biomedical Engineering.</strong> With the Protein-Protein Interaction Network, graph convolution and relation network for breast cancer subtype classification can be used and provide efficient results. In addition, a GCN-based model for polypharmacy side effects prediction. They can be used to model the drug and protein interaction network and separately deal with edges in different types.</p> <p><strong>Combinatorial optimization.</strong> Combinatorial optimization issues related to graphs comprise a collection of NP-hard challenges that draw significant interest from researchers across various disciplines. Certain well-known problems, such as the travelling salesman problem (TSP) and minimum spanning trees (MST), have seen numerous heuristic approaches developed. Lately, employing deep neural networks to tackle these problems has gained popularity, with some solutions additionally utilizing graph neural networks due to their inherent graph characteristics.</p> <p><strong>Traffic networks.</strong> Forecasting traffic conditions is a difficult undertaking due to the dynamic nature of traffic networks and their intricate dependencies. Certain studies have integrated GNNs with LSTMs to effectively model both spatial and temporal relationships. Moreover, the use of ST-Conv blocks, which incorporate spatial and temporal convolution layers along with residual connections and bottleneck techniques, has demonstrated excellent outcomes.</p> <p>Other ones are the distribution of water or electricity as they require the shortest distance problem and graph framework poses as a great tool for solution.</p> <h3 id="adjacency-matrix">Adjacency matrix</h3> \[A = \begin{array}{c|cccccccc} &amp; N_1 &amp; N_2 &amp; N_3 &amp; N_4 &amp; N_5 &amp; N_6 &amp; N_7 &amp; N_8 \\ \hline N_1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ N_2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ N_3 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\ N_4 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ N_5 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ N_6 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ N_7 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ N_8 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \end{array}\] <h3 id="degree-matrix">Degree matrix</h3> \[D = \begin{pmatrix} 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}\] <h3 id="laplacian-matrix">Laplacian matrix</h3> \[L = \begin{array}{c|cccccccc} &amp; N_1 &amp; N_2 &amp; N_3 &amp; N_4 &amp; N_5 &amp; N_6 &amp; N_7 &amp; N_8 \\ \hline N_1 &amp; 2 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ N_2 &amp; -1 &amp; 3 &amp; 0 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\ N_3 &amp; -1 &amp; 0 &amp; 3 &amp; 0 &amp; 0 &amp; -1 &amp; -1 &amp; 0 \\ N_4 &amp; 0 &amp; -1 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; -1 \\ N_5 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; -1 \\ N_6 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 2 &amp; -1 &amp; 0 \\ N_7 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 &amp; 0 \\ N_8 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 2 \end{array}\] <p>The second part of the code implements a diffusion process on a graph using a symmetrically normalized adjacency matrix. This process simulates how information propagates across the nodes of the graph over multiple iterations.</p> <h2 id="step-by-step-explanation">Step-by-step explanation</h2> <h3 id="modified-adjacency-matrix-a">Modified adjacency matrix $A^*$</h3> <p>The adjacency matrix $A$ represents the structure of the graph, where</p> \[A_{ij} = \begin{cases} 1, &amp; \text{if there is an edge between nodes } i \text{ and } j, \\ 0, &amp; \text{otherwise}. \end{cases}\] <p>To ensure that each node preserves part of its own information during propagation, self-loops are added to the graph. This leads to the modified adjacency matrix \(A^* = A + I\) where $I$ denotes the identity matrix.</p> <h2 id="degree-matrix-d">Degree matrix $D^*$</h2> <p>The degree matrix (D^*) is a diagonal matrix whose entries correspond to the sum of each row of (A^*), i.e.,</p> \[D^*_{ii} = \sum_{j} A^\*\_{ij}.\] <h2 id="symmetrically-normalized-adjacency-matrix-tildea">Symmetrically normalized adjacency matrix $\tilde{A}$</h2> <p>Rather than using the modified adjacency matrix directly, a symmetric normalization is applied, inspired by spectral graph theory:</p> \[\tilde{A} = D^{\*-\frac{1}{2}} A^\* D^{\*-\frac{1}{2}}.\] <p>This normalization balances the influence of nodes with different degrees and ensures numerical stability during diffusion.</p> <h2 id="propagation-of-information">Propagation of information</h2> <p>A diffusion (or message-passing) process is simulated over the graph. At each iteration, node representations are updated according to \(H^{(t+1)} = \tilde{A} H^{(t)},\) where $H^{(t)}$ denotes the node information at iteration $t$. The representations at each iteration are stored for later analysis and visualization.</p> <h2 id="visualization">Visualization</h2> <p>The diffusion process is visualized using <strong>networkx</strong> for graph construction and <strong>matplotlib.animation</strong> for animation. Nodes are colored according to their information values, illustrating how the initial signal at node 0 spreads through the network over time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/graphs.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p><em>Code implementations and experiments related to these algorithms are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[Graphs, Adjacency Matrix, Graph Neural Nets]]></summary></entry><entry><title type="html">K-Nearest Neighbor (KNN) – How your closest connections define you!</title><link href="https://olivierkanamugire.github.io/blog/2024/knn/" rel="alternate" type="text/html" title="K-Nearest Neighbor (KNN) – How your closest connections define you!"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2024/knn</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2024/knn/"><![CDATA[<p>K-Nearest Neighbor (KNN) is an intuitive, supervised machine learning algorithm named a lazy learner. That is because rather than constructing an explicit training model, KNN makes predictions by directly relying on the surrounding data points—its <code class="language-plaintext highlighter-rouge">neighbors</code>—to classify or determine the label of a new instance. This behavior can be analogized to human interactions: to understand someone, we often observe their social circle, as their friends tend to influence or describe who they are. In the context of KNN, the challenge lies in determining the optimal number of neighbors (k) to consider in order to best describe or classify a new point.</p> <p>From a mathematical perspective, KNN determines “closeness” by calculating <code class="language-plaintext highlighter-rouge">distances</code> between data points. Machines operate on numerical input, so distance metrics serve as a foundation for determining which neighbors are closest. Various distance measures exist, such as the Euclidean distance (L2 norm), Manhattan distance (L1 norm), and others. Selecting the appropriate distance metric is critical, as it influences the algorithm’s performance and suitability for specific problems.</p> <p>In this work, we provide a complete implementation of the KNN algorithm from scratch. We define custom functions to compute the necessary distances, identify neighbors, and make predictions, offering a comprehensive understanding of the model’s inner workings.</p> <h2 id="custom-sum-function">custom sum function</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fixed_custom_sum</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># Sum of all elements
</span>        <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">row</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Column-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Row-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid axis. Use 0 for columns or 1 for rows.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="sorting">sorting</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

</code></pre></div></div> <h2 id="distance">distance</h2> <p>Now we define euclidean distance we will utilize. \(d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\)</p> <p>This is very nice distance computing because it is implemented as vector computation. Therefore,By avoiding loops and using vectorized operations for distance calculation, we significantly improved the performance of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
    <span class="n">num_train_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span>
    <span class="n">num_test_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">testdata</span><span class="p">)</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_test_samples</span><span class="p">):</span>
        <span class="n">test_sample</span> <span class="o">=</span> <span class="n">testdata</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">test_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train_samples</span><span class="p">):</span>
            <span class="n">train_sample</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">squared_diff</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">train_sample</span><span class="p">,</span> <span class="n">test_sample</span><span class="p">)]</span>
            <span class="n">test_distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">custom_sum</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
        <span class="n">distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">test_distances</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distances</span>
</code></pre></div></div> <h2 id="majority-vote-for-classification">Majority vote for classification</h2> <p>Here, we choose the class that represents the majority of the closest neighbors, because the labels of the nearest neighbors collectively determine the final prediction for the test point.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">majority_vote</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">label_count</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Find the label with the maximum count
</span>    <span class="n">max_count</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">most_common_label</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">max_count</span><span class="p">:</span>
            <span class="n">max_count</span> <span class="o">=</span> <span class="n">count</span>
            <span class="n">most_common_label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">return</span> <span class="n">most_common_label</span>
</code></pre></div></div> <h2 id="knn-implementation-and-its-instantiation">KNN implementation and its instantiation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">traindata</span> <span class="o">=</span> <span class="n">traindata</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trainclass</span> <span class="o">=</span> <span class="n">trainclass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">)</span>

        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">distances</span><span class="p">:</span>
            <span class="n">k_indices</span> <span class="o">=</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">d</span><span class="p">)[:</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">]</span>
            <span class="n">k_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">trainclass</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
            <span class="n">k_nearest_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">majority_vote</span><span class="p">(</span><span class="n">k_labels</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">k_nearest_labels</span>
</code></pre></div></div> <p>Contribution</p> <p>No Libraries for Core Algorithm: This project is a pure implementation of KNN, providing a deeper understanding of how the algorithm works under the hood.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[KNN algorithm from scratch]]></summary></entry></feed>