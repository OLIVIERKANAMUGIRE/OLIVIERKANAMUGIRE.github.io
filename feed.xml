<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://olivierkanamugire.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://olivierkanamugire.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-31T20:07:20+00:00</updated><id>https://olivierkanamugire.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Nonlinear Support Vector Machine (SVM) with Polynomial Kernel</title><link href="https://olivierkanamugire.github.io/blog/2025/svm/" rel="alternate" type="text/html" title="Nonlinear Support Vector Machine (SVM) with Polynomial Kernel"/><published>2025-12-20T00:00:00+00:00</published><updated>2025-12-20T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/svm</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/svm/"><![CDATA[<h2 id="introduction-to-support-vector-machines">Introduction to Support Vector Machines</h2> <p>Support Vector Machines (SVMs) are powerful supervised learning algorithms designed for classification and regression tasks. What makes SVMs special is their unique approach to finding decision boundaries. Instead of trying to fit a probabilistic model to the data, SVMs search for the <strong>optimal separating hyperplane</strong> that creates the largest possible margin between different classes.</p> <p>Think of it this way: if you’re drawing a line to separate two groups of points, you want that line to be as far away as possible from both groups. This “safety margin” helps the model generalize better to new, unseen data.</p> <hr/> <h2 id="the-core-idea-maximum-margin-principle">The Core Idea: Maximum Margin Principle</h2> <p>The fundamental principle behind SVM is <strong>margin maximization</strong>. The margin is the perpendicular distance from the decision boundary (hyperplane) to the nearest data points from each class. These nearest points are called <strong>support vectors</strong> because they literally “support” or define the decision boundary.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/svm_overall-480.webp 480w,/assets/img/svm_overall-800.webp 800w,/assets/img/svm_overall-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/svm_overall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="why-maximize-the-margin">Why Maximize the Margin?</h3> <p>A larger margin provides:</p> <ul> <li><strong>Better generalization</strong>: The model is less sensitive to small variations in the data</li> <li><strong>Robustness</strong>: New data points are less likely to be misclassified</li> <li><strong>Geometric intuition</strong>: The decision is based on clear separation, not just probability</li> </ul> <h3 id="the-mathematical-formulation">The Mathematical Formulation</h3> <p>For a dataset with features <strong>$x$</strong> and binary labels <strong>$y$</strong> (where $y = +1$ or $y = -1$), we want to find a hyperplane defined by weights <strong>$w$</strong> and bias <strong>$b$</strong> such that:</p> \[\min_{\mathbf{w}, b} \; \frac{1}{2}\|\mathbf{w}\|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1\] <p><strong>Breaking this down:</strong></p> <ul> <li> <table> <tbody> <tr> <td>The term $\frac{1}{2}</td> <td>w</td> <td>^2$ measures the inverse of the margin (smaller <strong>w</strong> means larger margin)</td> </tr> </tbody> </table> </li> <li>The constraint $y_i(w^TX_i + b) \ge 1$ ensures all points are correctly classified with at least distance 1 from the boundary</li> <li>Only the support vectors (points closest to the boundary) affect the solution</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/svm_vid.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Illustration of the maximum margin principle. The optimal hyperplane is determined solely by the support vectors (the points touching the margin boundaries), while other points have no influence on the decision boundary. </div> <hr/> <h2 id="handling-real-world-data-soft-margin-svm">Handling Real-World Data: Soft Margin SVM</h2> <p>In practice, data is rarely perfectly separable. Classes often overlap due to noise, outliers, or the inherent nature of the problem. This is where <strong>soft margin SVM</strong> comes in.</p> <h3 id="introducing-slack-variables">Introducing Slack Variables</h3> <p>Soft margin SVM introduces slack variables $(\xi_i)$ that allow some points to violate the margin constraint. The optimization problem becomes:</p> \[\min_{\mathbf{w}, b, \xi} \; \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{m}\xi_i \quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1 - \xi_i, \; \xi_i \ge 0\] <h3 id="the-regularization-parameter-c">The Regularization Parameter C</h3> <p>The parameter <strong>C</strong> controls the trade-off between margin maximization and classification errors:</p> <ul> <li><strong>Large C</strong> (e.g., $C = 1000$): <ul> <li>Penalizes misclassifications heavily</li> <li>Results in narrower margins</li> <li>Lower bias, higher variance</li> <li>Risks overfitting</li> </ul> </li> <li><strong>Small C</strong> (e.g., $C = 0.1$): <ul> <li>Allows more margin violations</li> <li>Results in wider margins</li> <li>Higher bias, lower variance</li> <li>Better generalization</li> </ul> </li> </ul> <p><strong>Practical tip</strong>: Start with $C = 1.0$ and tune using cross-validation.</p> <hr/> <h2 id="the-kernel-trick-unlocking-nonlinear-classification">The Kernel Trick: Unlocking Nonlinear Classification</h2> <p>This is where SVM becomes truly powerful. Many real-world problems cannot be solved with a straight line (or hyperplane). The data might be arranged in circles, spirals, or other complex patterns.</p> <h3 id="what-is-the-kernel-trick">What is the Kernel Trick?</h3> <p>The kernel trick is a mathematical technique that allows us to:</p> <ol> <li>Implicitly map data to a higher-dimensional space</li> <li>Find a linear separator in that high-dimensional space</li> <li><strong>Never actually compute the high-dimensional coordinates</strong></li> </ol> <p>Instead of explicitly transforming each data point $\phi(x)$, we use a kernel function $K(x_1, x_2)$ that computes the inner product in the transformed space directly.</p> <h3 id="common-kernel-functions">Common Kernel Functions</h3> <h4 id="1-linear-kernel">1. Linear Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \mathbf{x}_1^\top \mathbf{x}_2\] <ul> <li>Use case: Data is already linearly separable</li> <li>Fastest to compute</li> </ul> <h4 id="2-polynomial-kernel">2. Polynomial Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = (1 + \mathbf{x}_1^\top \mathbf{x}_2)^d\] <ul> <li>Parameter: degree <strong>d</strong> (typically 2-5)</li> <li>Use case: Data has polynomial decision boundaries</li> <li>Our implementation uses this kernel!</li> </ul> <h4 id="3-radial-basis-function-rbfgaussian-kernel">3. Radial Basis Function (RBF/Gaussian) Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_2\|^2\right)\] <ul> <li>Parameter: $\gamma$ (controls influence radius)</li> <li>Use case: Most versatile, works for many nonlinear patterns</li> <li>Default choice when in doubt</li> </ul> <h4 id="4-sigmoid-kernel">4. Sigmoid Kernel</h4> \[K(\mathbf{x}_1, \mathbf{x}_2) = \tanh(\alpha \mathbf{x}_1^\top \mathbf{x}_2 + c)\] <ul> <li>Use case: Similar to neural network activation</li> <li>Less commonly used</li> </ul> <h3 id="how-the-kernel-trick-works">How the Kernel Trick Works</h3> <p>Consider a simple example with polynomial kernel of degree 2:</p> <ul> <li>Original space: 2D points $(x_1, x_2)$</li> <li>Transformed space: 6D $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$</li> <li>Direct computation: $O(d^2)$ operations</li> <li>Kernel computation: $O(d)$ operations</li> </ul> <p>The kernel computes the same result as the inner product in the transformed space, but much more efficiently!</p> <hr/> <h2 id="when-to-use-svms">When to Use SVMs</h2> <h3 id="svms-excel-when">SVMs Excel When:</h3> <ol> <li><strong>Small to medium-sized datasets</strong> (hundreds to thousands of samples) <ul> <li>Computational cost grows with dataset size</li> <li>Training time is $O(n^2)$ to $O(n^3)$</li> </ul> </li> <li><strong>High-dimensional feature spaces</strong> (many features relative to samples) <ul> <li>SVMs are less prone to overfitting in high dimensions</li> <li>Effective even when features » samples</li> </ul> </li> <li><strong>Clear margin separation exists</strong> <ul> <li>SVMs naturally find the best separating boundary</li> <li>Robust to outliers (with proper C tuning)</li> </ul> </li> <li><strong>Interpretability matters</strong> <ul> <li>Decision boundary is defined by support vectors</li> <li>Can identify which samples are most important</li> </ul> </li> </ol> <h3 id="consider-alternatives-when">Consider Alternatives When:</h3> <ul> <li>Dataset is very large (&gt;100,000 samples) $\to$ Use logistic regression or neural networks</li> <li>Data is very noisy with no clear separation $\to$ Try ensemble methods</li> <li>You need probability estimates $\to$ Use logistic regression or calibrated classifiers</li> <li>Real-time prediction speed is critical $\to$ Simpler models may be faster</li> </ul> <hr/> <h2 id="implementation-from-scratch">Implementation from Scratch</h2> <p>Now let’s implement a nonlinear SVM using Python! We’ll use the <strong>polynomial kernel</strong> and solve the optimization problem using quadratic programming.</p> <h3 id="step-1-the-polynomial-kernel-function">Step 1: The Polynomial Kernel Function</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># libraries
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">cvxopt</span> <span class="kn">import</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">solvers</span>

<span class="c1"># mathematical equation for polynomial kernel
</span><span class="k">def</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">.</span><span class="n">T</span><span class="p">))</span> <span class="o">**</span> <span class="n">degree</span>
</code></pre></div></div> <p><strong>Why add the +1?</strong> The constant term ensures that the kernel includes lower-degree polynomial terms as well. For example, with degree=2, we get terms involving $x_1^2, x_2^2, x_1x_2, x_1, x_2$, and the constant.</p> <h3 id="step-2-training-the-nonlinear-svm">Step 2: Training the Nonlinear SVM</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Train a nonlinear SVM with a polynomial kernel using Quadratic Programming.
    </span><span class="sh">"""</span>

    <span class="c1"># Convert labels from {1, 2} to {+1, -1} for mathematical convenience
</span>    <span class="n">trainclass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">#  Compute the Kernel Matrix:  K[i,j] represents the kernel similarity between sample i and sample j
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">traindata</span><span class="p">)</span>

    <span class="c1"># Set up the Quadratic Programming problem
</span>    <span class="c1"># We're solving: min (1/2)α^T P α + q^T α  subject to: Gα ≤ h and Aα = b
</span>    <span class="c1"># P matrix: (y_i * y_j) * K(x_i, x_j)
</span>    <span class="n">P</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">trainclass</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># q vector: all -1s (from the dual formulation)
</span>    <span class="n">q</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

    <span class="c1"># G and h: inequality constraints (α_i ≥ 0). We use -I and 0 to represent α_i ≥ 0
</span>    <span class="n">G</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

    <span class="c1"># A and b: equality constraint (Σ α_i * y_i = 0)
</span>    <span class="n">A</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">trainclass</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="c1">#  Solve the QP problem, we suppress solver output when option is False.
</span>    <span class="n">solvers</span><span class="p">.</span><span class="n">options</span><span class="p">[</span><span class="sh">'</span><span class="s">show_progress</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">solvers</span><span class="p">.</span><span class="nf">qp</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># Extract Lagrange multipliers
</span>    <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">])</span>

    <span class="c1"># Identify support vectors: Support vectors have $\alpha &gt; 0$ (choose a small threshold for numerical stability)
</span>    <span class="n">sv_idx</span> <span class="o">=</span> <span class="n">alphas</span> <span class="o">&gt;</span> <span class="mf">1e-5</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>
    <span class="n">svs</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>
    <span class="n">sv_classes</span> <span class="o">=</span> <span class="n">trainclass</span><span class="p">[</span><span class="n">sv_idx</span><span class="p">]</span>

    <span class="c1">#  Compute the bias term w0: we use the KKT conditions: for support vectors on the margin,
</span>    <span class="c1"># y_i * (Σ α_j * y_j * K(x_i, x_j) + w0) = 1
</span>    <span class="c1"># Therefore: w0 = y_i - Σ α_j * y_j * K(x_i, x_j)
</span>    <span class="c1"># We average over all support vectors for numerical stability
</span>    <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">([</span>
        <span class="n">sv_classes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span> <span class="o">*</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">svs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">svs</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">))</span>
    <span class="p">])</span>

    <span class="k">return</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span>
</code></pre></div></div> <p><strong>Key concepts in the code:</strong></p> <ol> <li><strong>Quadratic Programming</strong>: SVM training is formulated as a QP problem in the dual form</li> <li><strong>Support Vectors</strong>: Only points with α &gt; 0 actually contribute to the decision boundary</li> <li><strong>Kernel Matrix</strong>: All pairwise similarities are precomputed for efficient optimization</li> <li><strong>Bias Computation</strong>: Averaged over support vectors for better numerical stability</li> </ol> <h3 id="step-3-making-predictions">Step 3: Making Predictions</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_svm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Make predictions using the trained SVM.

    inputs:

    X : ndarray of shape (n_samples, n_features) / validationo set array
    svs : ndarray - Support vectors from training
    sv_classes : ndarray - Labels of support vectors
    alphas : ndarray -  Lagrange multipliers
    w0 : float -   Bias term

    output:

    predictions : ndarray - predicted class labels (+1 or -1)
    decision_scores : ndarray - raw decision function values (distance from boundary)
    </span><span class="sh">"""</span>
    <span class="c1"># Compute kernel between test points and support vectors
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">svs</span><span class="p">)</span>

    <span class="c1"># Decision function: f(x) = Σ α_i * y_i * K(x, x_i) + w0
</span>    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span><span class="p">)</span> <span class="o">+</span> <span class="n">w0</span>

    <span class="c1"># Predictions: sign of decision function
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">decision_scores</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">decision_scores</span>
</code></pre></div></div> <h3 id="step-4-visualizing-the-decision-boundary">Step 4: Visualizing the Decision Boundary</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the decision boundary, margins, and support vectors.

    This visualization helps understand:
    - How the SVM separates the classes
    - Which points are support vectors
    - The shape of the nonlinear decision boundary
    </span><span class="sh">"""</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># Plot the training data
</span>    <span class="c1"># Class 1 in blue, Class 2 in red
</span>    <span class="n">class1_mask</span> <span class="o">=</span> <span class="n">trainclass</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">class2_mask</span> <span class="o">=</span> <span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="n">class1_mask</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traindata</span><span class="p">[</span><span class="n">class1_mask</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Class 1</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="n">class2_mask</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traindata</span><span class="p">[</span><span class="n">class2_mask</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Class 2</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="c1"># Highlight support vectors with black circles
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">svs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">svs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">facecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Support Vectors</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Create a fine grid to evaluate the decision function
</span>    <span class="n">xlim</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">()</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">))</span>

    <span class="c1"># Evaluate decision function on every point in the grid
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()]</span>
    <span class="n">K_grid</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">svs</span><span class="p">)</span>
    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">K_grid</span><span class="p">,</span> <span class="n">alphas</span> <span class="o">*</span> <span class="n">sv_classes</span><span class="p">)</span> <span class="o">+</span> <span class="n">w0</span>
    <span class="n">decision_scores</span> <span class="o">=</span> <span class="n">decision_scores</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Plot the decision boundary (where decision_score = 0)
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">'</span><span class="s">solid</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Plot the margins (where decision_score = ±1)
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">'</span><span class="s">dashed</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="c1"># Fill the decision regions with light colors
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">decision_scores</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">],</span>
                 <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">lightblue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">lightcoral</span><span class="sh">'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Nonlinear SVM with Polynomial Kernel</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">best</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="step-5-putting-it-all-together">Step 5: Putting It All Together</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Load your data (replace with actual data loading)
</span>    <span class="c1"># Example of data set ono my github:
</span>    <span class="c1"># traindata = np.loadtxt('t030.csv', delimiter=',', usecols=(0,1))
</span>    <span class="c1"># trainclass = np.loadtxt('t030.csv', delimiter=',', usecols=(2,))
</span>
    <span class="c1">#Famous Example of cirular rings to show importance of kernel trick
</span>
    <span class="c1"># reproducibility
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Generate circular data
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">radius1</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">radius2</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="c1"># Inner circle (class 1)
</span>    <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">radius1</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">([</span><span class="n">r1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta1</span><span class="p">),</span> <span class="n">r1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta1</span><span class="p">)])</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Outer ring (class 2)
</span>    <span class="n">theta2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">radius1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">radius2</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">([</span><span class="n">r2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta2</span><span class="p">),</span> <span class="n">r2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta2</span><span class="p">)])</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">traindata</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">])</span>
    <span class="n">trainclass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">])</span>

    <span class="c1"># Train the SVM
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Nonlinear SVM...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of support vectors: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span><span class="si">}</span><span class="s"> out of </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span><span class="si">}</span><span class="s"> training samples</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Bias term (w0): </span><span class="si">{</span><span class="n">w0</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Visualize
</span>    <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>

    <span class="c1"># Make predictions on training data
</span>    <span class="n">predictions</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="nf">predict_svm</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">trainclass</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training accuracy: </span><span class="si">{</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <p>Results from that randomly generated samples with rings of points: famous example showing importance of kernel trick</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/svm_image-480.webp 480w,/assets/img/svm_image-800.webp 800w,/assets/img/svm_image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/svm_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="understanding-the-output">Understanding the Output</h2> <p>When you run this code, you’ll see:</p> <ol> <li> <p><strong>Support Vectors</strong>: Typically only a small fraction of training points become support vectors. These are the “critical” points that define the boundary.</p> </li> <li> <p><strong>Decision Boundary</strong>: The black solid line shows where the classifier switches between classes (decision_score = 0).</p> </li> <li> <p><strong>Margins</strong>: The dashed lines show the margin boundaries (decision_score = $\pm 1$). Support vectors lie on or within these margins.</p> </li> <li> <p><strong>Decision Regions</strong>: The shaded areas show which class the SVM predicts for each region of space.</p> </li> </ol> <hr/> <h2 id="hyperparameter-tuning-tips">Hyperparameter Tuning Tips</h2> <h3 id="comparative-analysis-choosing-c-regularization-parameter">Comparative analysis: Choosing C (Regularization Parameter)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Try different C values
</span><span class="n">C_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>
    <span class="n">svs</span><span class="p">,</span> <span class="n">sv_classes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="nf">svm_nonlinear</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">C=</span><span class="si">{</span><span class="n">C</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span><span class="si">}</span><span class="s"> support vectors</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><strong>Small C</strong> $(0.01-0.1)$: More support vectors, wider margin, better generalization</li> <li><strong>Large C</strong> $(10-100)$: Fewer support vectors, narrower margin, risk of overfitting</li> </ul> <h3 id="choosing-polynomial-degree">Choosing Polynomial Degree</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Modify the kernel function to accept different degrees
</span><span class="k">def</span> <span class="nf">svm_with_degree</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
    <span class="c1"># Modify polynomial_kernel calls to use specified degree
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">polynomial_kernel</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">traindata</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1">#  training codes ...
</span></code></pre></div></div> <ul> <li><strong>degree=2</strong>: Good starting point for most problems</li> <li><strong>degree=3-4</strong>: More complex boundaries, risk of overfitting</li> <li><strong>degree&gt;5</strong>: Rarely useful, very high computational cost</li> </ul> <hr/> <h2 id="common-pitfalls-and-solutions">Common Pitfalls and Solutions</h2> <h3 id="1-numerical-instability">1. Numerical Instability</h3> <p><strong>Problem</strong>: Kernel values become too large or too small <strong>Solution</strong>: Feature normalization</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">traindata</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span>
</code></pre></div></div> <h3 id="2-all-points-become-support-vectors">2. All Points Become Support Vectors</h3> <p><strong>Problem</strong>: C is too large or data has no clear separation <strong>Solution</strong>: Reduce C or try a different kernel</p> <h3 id="3-poor-performance">3. Poor Performance</h3> <p><strong>Problem</strong>: Wrong kernel choice or bad hyperparameters <strong>Solution</strong>: Use cross-validation to tune C and kernel parameters</p> <h3 id="4-slow-training">4. Slow Training</h3> <p><strong>Problem</strong>: Dataset is too large <strong>Solution</strong>: Consider using scikit-learn’s SVC with better optimization or subsample the data</p> <hr/> <h2 id="extending-this-implementation">Extending This Implementation</h2> <p>Want to take this further? Try:</p> <ol> <li><strong>Implement RBF kernel</strong>: Replace polynomial with Gaussian kernel</li> <li><strong>Add cross-validation</strong>: Use k-fold CV to select optimal C</li> <li><strong>Multi-class classification</strong>: Extend to handle more than two classes using one-vs-rest or one-vs-one</li> <li><strong>Feature importance</strong>: Analyze which support vectors contribute most</li> <li><strong>Compare with sklearn</strong>: Validate your implementation against scikit-learn</li> </ol> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Support Vector Machines provide an elegant combination of:</p> <ul> <li><strong>Geometric intuition</strong> (maximum margin principle)</li> <li><strong>Mathematical rigor</strong> (convex optimization)</li> <li><strong>Practical power</strong> (kernel trick for nonlinearity)</li> </ul> <p>By implementing SVM from scratch, we gain deep insights into:</p> <ul> <li>How the optimization problem is formulated</li> <li>Why support vectors are special</li> <li>How kernels enable nonlinear classification without explicit feature transformation</li> </ul> <p>While libraries like scikit-learn offer highly optimized implementations, understanding the fundamentals helps you make better choices about when and how to apply SVMs in practice.</p> <hr/> <h2 id="references-and-further-reading">References and Further Reading</h2> <ul> <li><strong>Original SVM Papers</strong>: Cortes, C., &amp; Vapnik, V. (1995) “Support-vector networks” Machine Learning, 20(3), 273-297. [The original paper introducing Support Vector Machines]</li> <li><strong>Kernel Methods</strong>: Schölkopf, B., &amp; Smola, A. J. (2002) “Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond” MIT Press, Cambridge, MA.</li> <li><strong>Practical Guide</strong>: Hsu, C. W., Chang, C. C., &amp; Lin, C. J. (2003) “A Practical Guide to Support Vector Classification” Technical Report, Department of Computer Science, National Taiwan University. Available: https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf [Essential practical guide for parameter selection and implementation]</li> <li><strong>Bayesian perceptive</strong>: “Pattern Recognition and Machine Learning” Springer, New York. Chapter 7: Sparse Kernel Machines [Modern treatment with Bayesian perspective] Cristianini, N., &amp; Shawe-Taylor, J. (2000)</li> <li><strong>Implementation</strong>: <a href="scikit-learn SVM documentation">https://scikit-learn.org/stable/modules/svm.html</a></li> </ul> <hr/> <h2 id="code-repository">Code Repository</h2> <p>The complete implementation with example datasets is available on <a href="GitHub -SVM">https://github.com/OLIVIERKANAMUGIRE/Support-Vector-Machines-from-Scratch</a></p> <p>Dataset used: <code class="language-plaintext highlighter-rouge">t030.csv</code> - A small nonlinearly separable 2D dataset for demonstration.</p> <hr/> <p><em>Happy learning! If you have questions or suggestions, feel free to reach out or open an issue on GitHub.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[Nonlinear support vector machine from scratch]]></summary></entry><entry><title type="html">Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction</title><link href="https://olivierkanamugire.github.io/blog/2025/svdappli/" rel="alternate" type="text/html" title="Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction"/><published>2025-01-12T00:00:00+00:00</published><updated>2025-01-12T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/svdappli</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/svdappli/"><![CDATA[<h2 id="introduction-and-objective">Introduction and Objective</h2> <p>Hyperspectral images contain rich spectral information across hundreds of wavelength bands, making them highly informative but also computationally expensive to store and process. Dimensionality reduction techniques are therefore essential for efficient analysis and visualization.</p> <p>In this work, <strong>Singular Value Decomposition (SVD)</strong> is applied to the well-known <strong>Indian Pines hyperspectral dataset</strong> to explore its structure and reduce its dimensionality while preserving most of the informative content. The main objective is to analyze the dataset, perform dimensionality reduction, and evaluate reconstruction quality.</p> <hr/> <h2 id="problem-statement">Problem Statement</h2> <p>Given the MATLAB file <code class="language-plaintext highlighter-rouge">indian_pines.mat</code>, the following tasks are performed:</p> <ol> <li>Load the hyperspectral data and visualize it in RGB format</li> <li>Generate a false-color image and analyze the spectral profile of a selected pixel</li> <li>Compute the total number of elements required to store the dataset</li> <li>Reshape the 3D hyperspectral cube into a 2D matrix</li> <li>Apply Singular Value Decomposition (SVD)</li> <li>Analyze cumulative explained variance and select an appropriate reconstruction rank</li> <li>Reconstruct the dataset using truncated SVD</li> <li>Compute dimensionality reduction percentage and reconstruction error (RMSE)</li> </ol> <hr/> <h2 id="methodology">Methodology</h2> <p>All computations and visualizations were performed using <strong>MATLAB</strong> [check corresponding github].</p> <h3 id="loading-and-visualization">Loading and Visualization</h3> <p>The hyperspectral image cube is loaded and visualized in RGB format. A false-color image is created by selecting specific spectral bands and applying contrast stretching. Pixel-level spectral behavior is analyzed, and the 3D data cube is reshaped into a 2D matrix using MATLAB’s built-in <code class="language-plaintext highlighter-rouge">reshape()</code> function.</p> <hr/> <h3 id="singular-value-decomposition">Singular Value Decomposition</h3> <p>After reshaping, Singular Value Decomposition is applied:</p> \[\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{V}^\top\] <p>The cumulative explained variance (CV) is computed as:</p> \[\text{CV} = \text{cumsum}(EV), \quad \text{where} \quad EV = \frac{\sigma_i^2}{\sum_i \sigma_i^2}\] <p>Here, $\sigma_i$ represents the singular values contained in the diagonal matrix $\mathbf{S}$. The reconstruction rank is selected based on the desired variance threshold.</p> <hr/> <h3 id="dimensionality-and-error-analysis">Dimensionality and Error Analysis</h3> <p>Using truncated SVD components, the dataset is reconstructed. The reduced dimensionality is computed as:</p> \[\text{Dim}_{\text{reduced}} = \text{elements}(U_k S_k) + \text{elements}(V_k)\] <p>The percentage reduction in dimensionality and the <strong>Root Mean Squared Error (RMSE)</strong> between the original and reconstructed datasets are used to quantify performance.</p> <hr/> <h2 id="results">Results</h2> <h3 id="image-loading-properties-and-visualization">Image Loading, Properties, and Visualization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img1-480.webp 480w,/assets/img/ADML/img1-800.webp 800w,/assets/img/ADML/img1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img2-480.webp 480w,/assets/img/ADML/img2-800.webp 800w,/assets/img/ADML/img2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The left image shows the RGB visualization, while the right image presents a false-color representation with contrast stretching.</p> <p><strong>Dataset properties:</strong></p> <p>Hypercube: DataCube: [145 × 145 × 220 double] Wavelength: [220 × 1 double]</p> <hr/> <h3 id="spectral-profile-of-a-pixel">Spectral Profile of a Pixel</h3> <p>A pixel located at <strong>row 60, column 70</strong> was selected to analyze its spectral behavior.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img3-480.webp 480w,/assets/img/ADML/img3-800.webp 800w,/assets/img/ADML/img3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The spectral profile indicates a significantly higher intensity around wavelength index 50 for this pixel.</p> <hr/> <h3 id="data-reshaping-and-storage-analysis">Data Reshaping and Storage Analysis</h3> <p>The total number of elements required to store the hyperspectral cube is:</p> \[145 \times 145 \times 220 = 4, 625, 500\] <p>The 3D matrix is reshaped into a 2D matrix of size:</p> \[(145 \times 145) \times 220 = 21 025 \times 220\] <hr/> <h3 id="svd-dimensionality-reduction-and-error-analysis">SVD, Dimensionality Reduction, and Error Analysis</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img5-480.webp 480w,/assets/img/ADML/img5-800.webp 800w,/assets/img/ADML/img5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The cumulative explained variance increases rapidly, indicating that a <strong>small number of singular values capture most of the information</strong>. Remarkably, <strong>99% of the variance is captured with rank = 2</strong>, resulting in a dimensionality reduction of approximately <strong>96.35%</strong>.</p> <hr/> <h3 id="reconstructed-image">Reconstructed Image</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img4-480.webp 480w,/assets/img/ADML/img4-800.webp 800w,/assets/img/ADML/img4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The reconstructed image retains nearly all structural information while using only a fraction of the original data dimensions.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>This experiment highlights the effectiveness of Singular Value Decomposition for hyperspectral image analysis. Despite the high dimensionality of the Indian Pines dataset, only a very low-rank approximation is required to preserve most of the information.</p> <p>Key observations include:</p> <ul> <li>Rapid growth of cumulative explained variance</li> <li>Significant reduction in storage requirements</li> <li>High-quality reconstruction with minimal loss</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Singular Value Decomposition proves to be a powerful and interpretable tool for dimensionality reduction in hyperspectral imaging. Its ability to compress data while preserving essential information makes it suitable for preprocessing, visualization, and further machine learning tasks.</p> <p>Future extensions may include:</p> <ul> <li>PCA comparison</li> <li>noise robustness analysis</li> <li>band selection strategies</li> <li>integration with classification models</li> </ul> <hr/> <p><em>Code and figures related to this analysis are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="linear-algebra"/><category term="svd"/><category term="hyperspectral"/><category term="dimensionality-reduction"/><summary type="html"><![CDATA[Dimensionality reduction of the Indian Pines hyperspectral dataset using Singular Value Decomposition (SVD)]]></summary></entry><entry><title type="html">Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction</title><link href="https://olivierkanamugire.github.io/blog/2025/svd_blog_extended/" rel="alternate" type="text/html" title="Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction"/><published>2025-01-12T00:00:00+00:00</published><updated>2025-01-12T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/svd_blog_extended</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/svd_blog_extended/"><![CDATA[<h2 id="introduction-to-hyperspectral-imaging">Introduction to Hyperspectral Imaging</h2> <p>Hyperspectral imaging is like having a camera with hundreds of colored filters. While a regular camera captures red, green, and blue (3 channels), a hyperspectral camera captures hundreds of narrow wavelength bands across the electromagnetic spectrum. This creates an image “cube” - two spatial dimensions and one spectral dimension.</p> <p><strong>Why is this useful?</strong> Different materials reflect and absorb light differently across wavelengths. For example:</p> <ul> <li>Healthy vegetation reflects strongly in near-infrared</li> <li>Different minerals have unique spectral signatures</li> <li>Water bodies absorb differently than soil</li> </ul> <p><strong>The challenge:</strong> A single hyperspectral image can contain millions of data points, making it expensive to store, transmit, and analyze. This is where <strong>dimensionality reduction</strong> becomes critical.</p> <hr/> <h2 id="what-is-singular-value-decomposition-svd">What is Singular Value Decomposition (SVD)?</h2> <h3 id="the-big-picture">The Big Picture</h3> <p>Imagine you have a large, complex dataset represented as a matrix. SVD is a mathematical technique that decomposes this matrix into three simpler matrices that, when multiplied together, reconstruct the original data. The magic is that <strong>most of the important information is concentrated in just a few components</strong>, allowing us to throw away the less important parts.</p> <p>Think of it like compressing a high-resolution image: you can often reduce file size by 90% while keeping the image looking almost identical to the human eye.</p> <h3 id="the-mathematical-framework">The Mathematical Framework</h3> <p>For any matrix $X$ of size $m\times n$, SVD decomposes it as:</p> \[\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top\] <p>Let’s break down each component:</p> <p><strong>U (Left Singular Vectors)</strong> - size m × m</p> <ul> <li>Columns are orthonormal vectors (perpendicular and unit length)</li> <li>Represent patterns in the <strong>rows</strong> of X (spatial patterns in our case)</li> <li>Think of these as “basis images” or “eigenimages”</li> </ul> <p><strong>$\Sigma$ (Singular Values)</strong> - size $m \times n$ (diagonal matrix)</p> <ul> <li>Diagonal elements $\sigma_1 \ge \sigma_2 \ge\sigma_3 \ge\sigma_4 \ge \ldots \ge 0$ (sorted in descending order)</li> <li>Measure the “strength” or “importance” of each component</li> <li>Large values = important patterns, small values = noise or fine details</li> <li>The rank $r$ is the number of non-zero singular values</li> </ul> <p><strong>V (Right Singular Vectors)</strong> - size $n \times n$</p> <ul> <li>Columns are orthonormal vectors</li> <li>Represent patterns in the <strong>columns</strong> of X (spectral patterns in our case)</li> <li>Show how different wavelengths relate to each other</li> </ul> <h3 id="why-svd-is-special">Why SVD is Special</h3> <p>SVD has unique mathematical properties that make it perfect for data analysis:</p> <ol> <li><strong>Unique and stable</strong>: For any matrix, the SVD is unique (up to sign)</li> <li><strong>Optimal approximation</strong>: Truncated SVD gives the best low-rank approximation in terms of minimizing error</li> <li><strong>Reveals structure</strong>: Singular values tell us the “intrinsic dimensionality” of the data</li> <li><strong>Works on any matrix</strong>: Unlike eigendecomposition, SVD works on non-square and non-symmetric matrices</li> </ol> <hr/> <h2 id="understanding-svd-through-intuition">Understanding SVD Through Intuition</h2> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Think of SVD as finding the “principal axes” of your data:</p> <ol> <li><strong>First component</strong> (largest $\sigma_1$): The direction of maximum variation in your data</li> <li><strong>Second component</strong> ($\sigma_2$): The direction of maximum variation orthogonal to the first</li> <li><strong>Third component</strong> ($\sigma_3$): Maximum variation orthogonal to both previous directions</li> <li>And so on…</li> </ol> <p>Each component is <strong>independent</strong> (orthogonal) to all others, so they capture different aspects of the data.</p> <h3 id="the-low-rank-approximation">The Low-Rank Approximation</h3> <p>Here’s the key insight: instead of using all components, we can use only the first k components:</p> \[\mathbf{X}_k = \mathbf{U}_k\mathbf{\Sigma}_k\mathbf{V}_k^\top\] <p>where:</p> <ul> <li><strong>U</strong>ₖ contains only the first k columns of <strong>U</strong></li> <li>$\Sigma$ contains only the first k singular values</li> <li><strong>V</strong>ₖ contains only the first k columns of <strong>V</strong></li> </ul> <p>This is called <strong>truncated SVD</strong> or <strong>rank-k approximation</strong>.</p> <p><strong>Why does this work?</strong> Because singular values are sorted by importance, the first few components capture most of the variance in the data, while later components often represent noise or insignificant details.</p> <hr/> <h2 id="svd-vs-pca-whats-the-connection">SVD vs PCA: What’s the Connection?</h2> <p>You might have heard of Principal Component Analysis (PCA). Here’s the relationship:</p> <p><strong>PCA and SVD are intimately related:</strong></p> <ul> <li>PCA finds directions of maximum variance by computing eigenvectors of the covariance matrix</li> <li>SVD directly decomposes the data matrix</li> <li><strong>For centered data, PCA and SVD give the same results</strong></li> </ul> <p>Specifically:</p> <ul> <li>The columns of <strong>V</strong> are the principal components</li> <li>The singular values $\sigma_i$ relate to eigenvalues: $\lambda_i = \sigma_i^2/(n-1)$</li> <li><strong>U</strong> gives the data projected onto principal components</li> </ul> <p><strong>Advantage of SVD over PCA:</strong></p> <ul> <li>More numerically stable</li> <li>Doesn’t require computing the covariance matrix (which can be large)</li> <li>Directly applicable to the data matrix</li> </ul> <hr/> <h2 id="measuring-information-content-explained-variance">Measuring Information Content: Explained Variance</h2> <p>How do we decide how many components to keep? We use <strong>explained variance</strong>.</p> <h3 id="computing-explained-variance">Computing Explained Variance</h3> <p>The variance explained by the i-th component is:</p> \[\text{EV}_i = \frac{\sigma_i^2}{\sum_{j=1}^{r} \sigma_j^2}\] <p><strong>Cumulative explained variance</strong> up to component k:</p> \[\text{CV}_k = \frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{j=1}^{r} \sigma_j^2}\] <p>This tells us: <strong>“What percentage of the total information is captured by the first k components?”</strong></p> <h3 id="choosing-the-number-of-components">Choosing the Number of Components</h3> <p>Common strategies:</p> <ol> <li><strong>Threshold method</strong>: Keep enough components to capture 95% or 99% of variance</li> <li><strong>Elbow method</strong>: Plot variance vs. component number, look for the “elbow” where adding more components gives diminishing returns</li> <li><strong>Cross-validation</strong>: Choose k that gives best performance on validation data</li> <li><strong>Fixed number</strong>: Use domain knowledge (e.g., “keep 10 components”)</li> </ol> <p>For our hyperspectral data, we’ll use the threshold method with 99% variance retention.</p> <hr/> <h2 id="application-to-hyperspectral-imaging">Application to Hyperspectral Imaging</h2> <h3 id="the-indian-pines-dataset">The Indian Pines Dataset</h3> <p>The Indian Pines dataset is a benchmark hyperspectral image collected over agricultural land in Indiana, USA. It contains:</p> <ul> <li><strong>Spatial dimensions</strong>: $145 \times 145$ pixels</li> <li><strong>Spectral dimension</strong>: 220 wavelength bands</li> <li><strong>Total elements</strong>: $145 \times 145 \times 220 = 4,625,500$ values</li> </ul> <p>This represents different crop types, forests, roads, and buildings, each with unique spectral signatures.</p> <h3 id="why-apply-svd-to-hyperspectral-data">Why Apply SVD to Hyperspectral Data?</h3> <p>Hyperspectral images are highly redundant because:</p> <ol> <li><strong>Spectral correlation</strong>: Adjacent wavelength bands are highly correlated</li> <li><strong>Spatial correlation</strong>: Nearby pixels often have similar spectra</li> <li><strong>Noise</strong>: Some bands contain primarily noise</li> </ol> <p>SVD exploits this redundancy to:</p> <ul> <li><strong>Compress</strong> the data for storage and transmission</li> <li><strong>Denoise</strong> by removing low-variance components</li> <li><strong>Visualize</strong> high-dimensional data</li> <li><strong>Preprocess</strong> for machine learning algorithms</li> </ul> <hr/> <h2 id="methodology-and-implementation">Methodology and Implementation</h2> <h3 id="1-data-loading-and-preprocessing">1: Data Loading and Preprocessing</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Load the hyperspectral data</span>
<span class="nb">load</span><span class="p">(</span><span class="s1">'indian_pines.mat'</span><span class="p">);</span>

<span class="c1">% Extract the datacube and wavelength information</span>
<span class="n">datacube</span> <span class="o">=</span> <span class="n">indian_pines</span><span class="o">.</span><span class="n">DataCube</span><span class="p">;</span>  <span class="c1">% 145 × 145 × 220</span>
<span class="n">wavelengths</span> <span class="o">=</span> <span class="n">indian_pines</span><span class="o">.</span><span class="n">Wavelength</span><span class="p">;</span>  <span class="c1">% 220 × 1</span>

<span class="c1">% Display dimensions</span>
<span class="p">[</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">bands</span><span class="p">]</span> <span class="o">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">datacube</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Image size: %d × %d pixels\n'</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Number of spectral bands: %d\n'</span><span class="p">,</span> <span class="n">bands</span><span class="p">);</span>
</code></pre></div></div> <h3 id="2-rgb-visualization">2: RGB Visualization</h3> <p>To visualize the hyperspectral cube as an RGB image, we select three representative bands:</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Select bands for RGB (approximate wavelengths)</span>
<span class="n">red_band</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span>    <span class="c1">% ~650 nm</span>
<span class="n">green_band</span> <span class="o">=</span> <span class="mi">27</span><span class="p">;</span>  <span class="err">#</span> <span class="o">~</span><span class="mi">550</span> <span class="n">nm</span>
<span class="n">blue_band</span> <span class="o">=</span> <span class="mi">17</span><span class="p">;</span>   <span class="err">#</span> <span class="o">~</span><span class="mi">450</span> <span class="n">nm</span>

<span class="c1">% Extract and normalize</span>
<span class="n">rgb_image</span> <span class="o">=</span> <span class="nb">cat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">red_band</span><span class="p">),</span> <span class="k">...</span>
                   <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">green_band</span><span class="p">),</span> <span class="k">...</span>
                   <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">blue_band</span><span class="p">));</span>
<span class="n">rgb_image</span> <span class="o">=</span> <span class="n">rgb_image</span> <span class="p">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">(:));</span>

<span class="c1">% Display</span>
<span class="nb">figure</span><span class="p">;</span>
<span class="nb">imshow</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'RGB Visualization of Hyperspectral Image'</span><span class="p">);</span>
</code></pre></div></div> <h3 id="3-false-color-composite">3: False-Color Composite</h3> <p>False-color images use non-visible wavelengths to highlight features:</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Select NIR, Red, Green for false-color (vegetation analysis)</span>
<span class="n">nir_band</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>   <span class="c1">% Near-infrared</span>
<span class="n">red_band</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span>
<span class="n">green_band</span> <span class="o">=</span> <span class="mi">27</span><span class="p">;</span>

<span class="c1">% Create false-color composite</span>
<span class="n">false_color</span> <span class="o">=</span> <span class="nb">cat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">nir_band</span><span class="p">),</span> <span class="k">...</span>
                     <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">red_band</span><span class="p">),</span> <span class="k">...</span>
                     <span class="n">datacube</span><span class="p">(:,:,</span><span class="n">green_band</span><span class="p">));</span>

<span class="c1">% Apply contrast stretching</span>
<span class="n">false_color</span> <span class="o">=</span> <span class="n">imadjust</span><span class="p">(</span><span class="n">false_color</span> <span class="p">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">false_color</span><span class="p">(:)));</span>

<span class="nb">figure</span><span class="p">;</span>
<span class="nb">imshow</span><span class="p">(</span><span class="n">false_color</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'False-Color Composite (NIR-R-G)'</span><span class="p">);</span>
</code></pre></div></div> <p><strong>Why false-color?</strong> Vegetation reflects strongly in near-infrared but absorbs red light. In false-color, healthy vegetation appears bright red!</p> <h3 id="4-spectral-profile-analysis">4: Spectral Profile Analysis</h3> <p>Examining the spectral signature of a single pixel:</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Select a pixel</span>
<span class="n">row_idx</span> <span class="o">=</span> <span class="mi">60</span><span class="p">;</span>
<span class="n">col_idx</span> <span class="o">=</span> <span class="mi">70</span><span class="p">;</span>

<span class="c1">% Extract spectral profile</span>
<span class="n">spectrum</span> <span class="o">=</span> <span class="nb">squeeze</span><span class="p">(</span><span class="n">datacube</span><span class="p">(</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="p">:));</span>

<span class="c1">% Plot</span>
<span class="nb">figure</span><span class="p">;</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">wavelengths</span><span class="p">,</span> <span class="n">spectrum</span><span class="p">,</span> <span class="s1">'LineWidth'</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Wavelength (nm)'</span><span class="p">);</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Reflectance'</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="nb">sprintf</span><span class="p">(</span><span class="s1">'Spectral Profile at Pixel (%d, %d)'</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">));</span>
<span class="nb">grid</span> <span class="n">on</span><span class="p">;</span>
</code></pre></div></div> <p>This reveals the unique spectral “fingerprint” of the material at that location.</p> <h3 id="5-data-reshaping">5: Data Reshaping</h3> <p>Convert the 3D cube into a 2D matrix for SVD:</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Original: 145 × 145 × 220 (rows × cols × bands)</span>
<span class="c1">% Reshape to: (145×145) × 220 = 21025 × 220</span>
<span class="c1">% Each row is a pixel, each column is a wavelength band</span>

<span class="n">X</span> <span class="o">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">datacube</span><span class="p">,</span> <span class="n">rows</span><span class="o">*</span><span class="n">cols</span><span class="p">,</span> <span class="n">bands</span><span class="p">);</span>

<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Original size: %d elements\n'</span><span class="p">,</span> <span class="n">rows</span><span class="o">*</span><span class="n">cols</span><span class="o">*</span><span class="n">bands</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Reshaped matrix: %d × %d\n'</span><span class="p">,</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">2</span><span class="p">));</span>
</code></pre></div></div> <p><strong>Understanding the reshape:</strong></p> <ul> <li>Each <strong>row</strong> represents one pixel’s complete spectrum</li> <li>Each <strong>column</strong> represents one wavelength across all pixels</li> <li>We’ve “unfolded” the spatial dimensions into rows</li> </ul> <h3 id="6-applying-svd">6: Applying SVD</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Perform SVD</span>
<span class="p">[</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="nb">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">'econ'</span><span class="p">);</span>
<span class="c1">% 'econ' produces economy-size decomposition: faster and uses less memory</span>

<span class="c1">% Extract singular values</span>
<span class="n">singular_values</span> <span class="o">=</span> <span class="nb">diag</span><span class="p">(</span><span class="n">S</span><span class="p">);</span>

<span class="c1">% Compute explained variance</span>
<span class="n">explained_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">singular_values</span><span class="o">.^</span><span class="mi">2</span><span class="p">)</span> <span class="p">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">singular_values</span><span class="o">.^</span><span class="mi">2</span><span class="p">);</span>
<span class="n">cumulative_var</span> <span class="o">=</span> <span class="nb">cumsum</span><span class="p">(</span><span class="n">explained_var</span><span class="p">);</span>

<span class="c1">% Plot</span>
<span class="nb">figure</span><span class="p">;</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">singular_values</span><span class="p">,</span> <span class="s1">'o-'</span><span class="p">,</span> <span class="s1">'LineWidth'</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Component Index'</span><span class="p">);</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Singular Value'</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Singular Values (Scree Plot)'</span><span class="p">);</span>
<span class="nb">grid</span> <span class="n">on</span><span class="p">;</span>

<span class="nb">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">cumulative_var</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">'LineWidth'</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Number of Components'</span><span class="p">);</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Cumulative Explained Variance (%)'</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Cumulative Variance Explained'</span><span class="p">);</span>
<span class="nb">grid</span> <span class="n">on</span><span class="p">;</span>
<span class="nb">yline</span><span class="p">(</span><span class="mi">99</span><span class="p">,</span> <span class="s1">'--r'</span><span class="p">,</span> <span class="s1">'99% Threshold'</span><span class="p">);</span>
</code></pre></div></div> <p><strong>Key observations:</strong></p> <ul> <li>Singular values drop rapidly (most information in first few components)</li> <li>Cumulative variance plateaus quickly (diminishing returns)</li> </ul> <h3 id="7selecting-reconstruction-rank">7:Selecting Reconstruction Rank</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Find minimum k for 99% variance</span>
<span class="n">variance_threshold</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">;</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">find</span><span class="p">(</span><span class="n">cumulative_var</span> <span class="o">&gt;=</span> <span class="n">variance_threshold</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'first'</span><span class="p">);</span>

<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Components needed for %.1f%% variance: %d\n'</span><span class="p">,</span> <span class="k">...</span>
        <span class="n">variance_threshold</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">k</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Variance captured: %.4f%%\n'</span><span class="p">,</span> <span class="n">cumulative_var</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">);</span>
</code></pre></div></div> <h3 id="8-low-rank-reconstruction">8: Low-Rank Reconstruction</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Truncate to first k components</span>
<span class="n">U_k</span> <span class="o">=</span> <span class="n">U</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">);</span>
<span class="n">S_k</span> <span class="o">=</span> <span class="n">S</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">);</span>
<span class="n">V_k</span> <span class="o">=</span> <span class="n">V</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">);</span>

<span class="c1">% Reconstruct</span>
<span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">U_k</span> <span class="o">*</span> <span class="n">S_k</span> <span class="o">*</span> <span class="n">V_k</span><span class="o">'</span><span class="p">;</span>

<span class="c1">% Reshape back to 3D cube</span>
<span class="n">datacube_reconstructed</span> <span class="o">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">X_reconstructed</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">bands</span><span class="p">);</span>
</code></pre></div></div> <p><strong>What we’ve done:</strong></p> <ul> <li>Kept only the first k columns of U and V</li> <li>Kept only the first k singular values from Σ</li> <li>Multiplied them to get an approximation of X</li> </ul> <h3 id="9-computing-dimensionality-reduction">9: Computing Dimensionality Reduction</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Original storage</span>
<span class="n">original_elements</span> <span class="o">=</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">bands</span><span class="p">;</span>

<span class="c1">% Reduced storage: U_k + S_k + V_k</span>
<span class="n">reduced_elements</span> <span class="o">=</span> <span class="nb">numel</span><span class="p">(</span><span class="n">U_k</span><span class="p">)</span> <span class="o">+</span> <span class="nb">numel</span><span class="p">(</span><span class="n">S_k</span><span class="p">)</span> <span class="o">+</span> <span class="nb">numel</span><span class="p">(</span><span class="n">V_k</span><span class="p">);</span>
<span class="c1">% Or equivalently: (rows*cols)*k + k*k + bands*k</span>

<span class="c1">% Reduction percentage</span>
<span class="n">reduction</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">reduced_elements</span><span class="p">/</span><span class="n">original_elements</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">;</span>

<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Original elements: %d\n'</span><span class="p">,</span> <span class="n">original_elements</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Reduced elements: %d\n'</span><span class="p">,</span> <span class="n">reduced_elements</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Dimensionality reduction: %.2f%%\n'</span><span class="p">,</span> <span class="n">reduction</span><span class="p">);</span>
</code></pre></div></div> <p><strong>Understanding the storage:</strong></p> <ul> <li>Original: Store entire 145×145×220 cube</li> <li>Reduced: Store U_k (21025×k) + S_k (k×k) + V_k (220×k)</li> <li>Huge savings when k « min(rows×cols, bands)</li> </ul> <h3 id="10-error-analysis">10: Error Analysis</h3> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Compute Root Mean Squared Error</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="nb">sqrt</span><span class="p">(</span><span class="nb">mean</span><span class="p">((</span><span class="n">X</span><span class="p">(:)</span> <span class="o">-</span> <span class="n">X_reconstructed</span><span class="p">(:))</span><span class="o">.^</span><span class="mi">2</span><span class="p">));</span>

<span class="c1">% Relative error</span>
<span class="n">relative_error</span> <span class="o">=</span> <span class="n">rmse</span> <span class="p">/</span> <span class="nb">sqrt</span><span class="p">(</span><span class="nb">mean</span><span class="p">(</span><span class="n">X</span><span class="p">(:)</span><span class="o">.^</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">;</span>

<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'RMSE: %.6f\n'</span><span class="p">,</span> <span class="n">rmse</span><span class="p">);</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'Relative error: %.4f%%\n'</span><span class="p">,</span> <span class="n">relative_error</span><span class="p">);</span>

<span class="c1">% Visualize reconstruction quality</span>
<span class="nb">figure</span><span class="p">;</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="nb">imshow</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Original RGB'</span><span class="p">);</span>

<span class="nb">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
<span class="n">rgb_reconstructed</span> <span class="o">=</span> <span class="nb">cat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">datacube_reconstructed</span><span class="p">(:,:,</span><span class="n">red_band</span><span class="p">),</span> <span class="k">...</span>
                           <span class="n">datacube_reconstructed</span><span class="p">(:,:,</span><span class="n">green_band</span><span class="p">),</span> <span class="k">...</span>
                           <span class="n">datacube_reconstructed</span><span class="p">(:,:,</span><span class="n">blue_band</span><span class="p">));</span>
<span class="n">rgb_reconstructed</span> <span class="o">=</span> <span class="n">rgb_reconstructed</span> <span class="p">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">rgb_reconstructed</span><span class="p">(:));</span>
<span class="nb">imshow</span><span class="p">(</span><span class="n">rgb_reconstructed</span><span class="p">);</span>
<span class="nb">title</span><span class="p">(</span><span class="nb">sprintf</span><span class="p">(</span><span class="s1">'Reconstructed RGB (k=%d)'</span><span class="p">,</span> <span class="n">k</span><span class="p">));</span>
</code></pre></div></div> <hr/> <h2 id="results-and-analysis">Results and Analysis</h2> <h3 id="image-loading-and-properties">Image Loading and Properties</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img1-480.webp 480w,/assets/img/ADML/img1-800.webp 800w,/assets/img/ADML/img1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img2-480.webp 480w,/assets/img/ADML/img2-800.webp 800w,/assets/img/ADML/img2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Left: RGB visualization using bands at approximately 650nm (red), 550nm (green), and 450nm (blue). Right: False-color composite using NIR-Red-Green bands with contrast stretching. Healthy vegetation appears bright red due to strong near-infrared reflection. </div> <p><strong>Dataset Properties:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hypercube Dimensions: 145 × 145 × 220
Total Elements: 4,625,500
Wavelength Range: 220 bands spanning visible to near-infrared
Storage Size: ~37 MB (double precision)
</code></pre></div></div> <hr/> <h3 id="spectral-signature-analysis">Spectral Signature Analysis</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img3-480.webp 480w,/assets/img/ADML/img3-800.webp 800w,/assets/img/ADML/img3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Spectral profile of pixel at position (60, 70). The significant peak around wavelength index 50 (approximately 650-700nm) indicates strong red/near-infrared reflection, characteristic of vegetation. </div> <p><strong>Interpretation:</strong></p> <ul> <li>The spectral profile shows the unique “fingerprint” of this pixel</li> <li>High reflectance in red edge and NIR suggests healthy vegetation</li> <li>Lower reflectance in blue/green (photosynthesis absorption)</li> <li>This demonstrates why hyperspectral imaging is powerful for classification</li> </ul> <hr/> <h3 id="data-transformation">Data Transformation</h3> <p><strong>Original 3D Structure:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Spatial dimensions: 145 × 145 pixels
Spectral dimension: 220 bands
Total: 4,625,500 elements
</code></pre></div></div> <p><strong>Reshaped 2D Matrix:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Rows (pixels): 21,025
Columns (bands): 220
Interpretation: Each row is a spectrum, each column is a spatial map
</code></pre></div></div> <p>This transformation is key for applying SVD, which operates on 2D matrices.</p> <hr/> <h3 id="svd-decomposition-results">SVD Decomposition Results</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img5-480.webp 480w,/assets/img/ADML/img5-800.webp 800w,/assets/img/ADML/img5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Cumulative explained variance vs. number of components. The rapid initial rise indicates that a small number of components capture most of the data's information content. </div> <p><strong>Key Findings:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Components for 99% variance</td> <td><strong>k = 2</strong></td> </tr> <tr> <td>Variance explained by first component</td> <td>~85%</td> </tr> <tr> <td>Variance explained by first two components</td> <td>99%</td> </tr> <tr> <td>Remaining 218 components</td> <td>1%</td> </tr> </tbody> </table> <p>This result is remarkable: <strong>99% of the information in 220 bands is captured by just 2 components!</strong></p> <p><strong>Why so few components?</strong></p> <ol> <li><strong>High spectral correlation</strong>: Adjacent wavelengths are very similar</li> <li><strong>Smooth spectral signatures</strong>: Most materials have smoothly varying spectra</li> <li><strong>Limited material diversity</strong>: Only a few land cover types in the scene</li> <li><strong>Noise</strong>: Some bands contain primarily noise (low signal)</li> </ol> <hr/> <h3 id="dimensionality-reduction-analysis">Dimensionality Reduction Analysis</h3> <p><strong>Storage Requirements:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original storage:
- Full datacube: 4,625,500 elements

Reduced storage (k=2):
- U_k: 21,025 × 2 = 42,050 elements
- S_k: 2 × 2 = 4 elements
- V_k: 220 × 2 = 440 elements
- Total: 42,494 elements

Reduction: 96.35%
Compression ratio: 108.9:1
</code></pre></div></div> <p><strong>Practical implications:</strong></p> <ul> <li><strong>Storage</strong>: Reduced from ~37 MB to ~0.34 MB</li> <li><strong>Transmission</strong>: 100× faster data transfer</li> <li><strong>Processing</strong>: Much faster computations on compressed data</li> <li><strong>Memory</strong>: Can handle larger datasets in limited RAM</li> </ul> <hr/> <h3 id="reconstructed-image-quality">Reconstructed Image Quality</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/img4-480.webp 480w,/assets/img/ADML/img4-800.webp 800w,/assets/img/ADML/img4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> RGB visualization of reconstructed hyperspectral cube using only 2 SVD components. Despite 96% reduction in dimensionality, the image retains all major structural features and most spectral information. </div> <p><strong>Reconstruction Error:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RMSE: ~0.0023 (very small!)
Relative error: &lt;1%
Visual quality: Excellent
</code></pre></div></div> <p><strong>What’s preserved:</strong></p> <ul> <li>Spatial structures (fields, roads, buildings)</li> <li>Spectral characteristics (material signatures)</li> <li>Statistical properties (means, variances)</li> </ul> <p><strong>What’s lost:</strong></p> <ul> <li>Fine spectral details</li> <li>High-frequency noise</li> <li>Minor spectral variations</li> </ul> <h2 id="mathematical-properties-of-svd">Mathematical Properties of SVD</h2> <h3 id="optimality-of-truncated-svd">Optimality of Truncated SVD</h3> <p>The rank-k approximation $X_k = U_k \Sigma_k V_k^T$ is optimal in the sense that it minimizes:</p> \[\|\mathbf{X} - \mathbf{X}_k\|_F = \min_{\text{rank}(\mathbf{Y})=k} \|\mathbf{X} - \mathbf{Y}\|_F\] <table> <tbody> <tr> <td>where $</td> <td>.</td> <td>_F$ is the Frobenius norm (sum of squared errors).</td> </tr> </tbody> </table> <p><strong>What this means:</strong> Among all possible rank-k matrices, truncated SVD gives the one closest to the original data. No other method can beat it!</p> <h3 id="eckart-young-mirsky-theorem">Eckart-Young-Mirsky Theorem</h3> <p>The reconstruction error is exactly:</p> \[\|\mathbf{X} - \mathbf{X}_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}\] <p>This tells us:</p> <ul> <li>Error depends only on discarded singular values</li> <li>If $\sigma_{k+1}, \ldots$ are small, error is small</li> <li>We can predict error before reconstruction!</li> </ul> <p><strong>Tip:</strong> Use economy SVD (<code class="language-plaintext highlighter-rouge">svd(X,'econ')</code> in MATLAB) for large datasets to save memory and time.</p> <hr/> <h2 id="practical-considerations">Practical Considerations</h2> <h3 id="when-svd-works-well">When SVD Works Well</h3> <p>SVD is most effective when:</p> <ol> <li><strong>High redundancy</strong>: Correlated features (like spectral bands)</li> <li><strong>Low intrinsic dimensionality</strong>: Data lies on a low-dimensional manifold</li> <li><strong>Smooth variations</strong>: Gradual changes rather than sharp transitions</li> <li><strong>Good signal-to-noise ratio</strong>: Noise doesn’t dominate signal</li> </ol> <p>Hyperspectral images typically satisfy all these conditions!</p> <h3 id="when-to-be-careful">When to Be Careful</h3> <p>SVD may struggle with:</p> <ol> <li><strong>Sparse data</strong>: Lots of zeros (use specialized sparse methods)</li> <li><strong>Non-linear structures</strong>: Curved manifolds (consider kernel PCA)</li> <li><strong>Discrete data</strong>: Categorical variables (inappropriate for SVD)</li> <li><strong>Very large matrices</strong>: Computational cost becomes prohibitive</li> </ol> <h3 id="alternative-methods">Alternative Methods</h3> <p>For comparison, other dimensionality reduction techniques include:</p> <p><strong>Linear methods:</strong></p> <ul> <li><strong>PCA</strong>: Essentially equivalent to SVD for centered data</li> <li><strong>NMF</strong> (Non-negative Matrix Factorization): Enforces non-negativity</li> <li><strong>ICA</strong> (Independent Component Analysis): Finds statistically independent components</li> <li><strong>LDA</strong> (Linear Discriminant Analysis): Supervised, maximizes class separation</li> </ul> <p><strong>Nonlinear methods:</strong></p> <ul> <li><strong>t-SNE</strong>: Great for visualization, poor for reconstruction</li> <li><strong>UMAP</strong>: Similar to t-SNE but faster</li> <li><strong>Autoencoders</strong>: Neural network-based compression</li> <li><strong>Kernel PCA</strong>: Nonlinear variant of PCA</li> </ul> <p>For hyperspectral imaging, <strong>SVD/PCA remains the gold standard</strong> due to simplicity, interpretability, and proven effectiveness.</p> <hr/> <h2 id="advanced-topics-and-extensions">Advanced Topics and Extensions</h2> <h3 id="incremental-svd">Incremental SVD</h3> <p>For streaming or large datasets that don’t fit in memory:</p> <ul> <li>Process data in batches</li> <li>Update SVD incrementally</li> <li>Maintain approximate low-rank representation</li> </ul> <h3 id="randomized-svd">Randomized SVD</h3> <p>For very large matrices:</p> <ul> <li>Use random projections for approximation</li> <li>Much faster than full SVD (O(mnk) instead of O(mn²))</li> <li>Controlled accuracy trade-off</li> </ul> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Randomized SVD (requires additional toolbox)</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="p">[</span><span class="n">U_approx</span><span class="p">,</span> <span class="n">S_approx</span><span class="p">,</span> <span class="n">V_approx</span><span class="p">]</span> <span class="o">=</span> <span class="n">rsvd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">);</span>
</code></pre></div></div> <h3 id="tensor-decomposition">Tensor Decomposition</h3> <p>Instead of reshaping to 2D, work directly with 3D tensor:</p> <ul> <li><strong>Tucker decomposition</strong>: Higher-order SVD</li> <li><strong>CP decomposition</strong>: CANDECOMP/PARAFAC</li> <li>Preserves multi-way structure</li> </ul> <h3 id="supervised-dimensionality-reduction">Supervised Dimensionality Reduction</h3> <p>If you have class labels (e.g., crop types):</p> <ul> <li><strong>Discriminant Analysis Feature Extraction (DAFE)</strong></li> <li><strong>Maximum Noise Fraction (MNF)</strong></li> <li>Optimize for classification performance</li> </ul> <h3 id="practical-applications">Practical Applications</h3> <p>This dimensionality reduction enables:</p> <p><strong>1. Classification</strong>: Train classifiers on compressed data</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Use reduced representation for classification</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">U_k</span> <span class="o">*</span> <span class="n">S_k</span><span class="p">;</span>  <span class="c1">% 21025×2 instead of 21025×220</span>
<span class="c1">% Train SVM, Random Forest, etc. on these features</span>
</code></pre></div></div> <p><strong>2. Anomaly Detection</strong>: Identify pixels that don’t fit the low-rank model</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Reconstruction error per pixel</span>
<span class="n">pixel_errors</span> <span class="o">=</span> <span class="nb">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_reconstructed</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
<span class="n">anomaly_map</span> <span class="o">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">pixel_errors</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">);</span>
</code></pre></div></div> <p><strong>3. Compression</strong>: Store/transmit compact representation</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% Save compressed version</span>
<span class="nb">save</span><span class="p">(</span><span class="s1">'indian_pines_compressed.mat'</span><span class="p">,</span> <span class="s1">'U_k'</span><span class="p">,</span> <span class="s1">'S_k'</span><span class="p">,</span> <span class="s1">'V_k'</span><span class="p">,</span> <span class="k">...</span>
     <span class="s1">'rows'</span><span class="p">,</span> <span class="s1">'cols'</span><span class="p">,</span> <span class="s1">'bands'</span><span class="p">,</span> <span class="s1">'wavelengths'</span><span class="p">);</span>
</code></pre></div></div> <p><strong>4. Visualization</strong>: Project to 2D/3D for human inspection</p> <div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% 2D scatter plot using first 2 components</span>
<span class="nb">scatter</span><span class="p">(</span><span class="n">U</span><span class="p">(:,</span><span class="mi">1</span><span class="p">),</span> <span class="n">U</span><span class="p">(:,</span><span class="mi">2</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="s1">'filled'</span><span class="p">);</span>
<span class="nb">xlabel</span><span class="p">(</span><span class="s1">'Component 1'</span><span class="p">);</span>
<span class="nb">ylabel</span><span class="p">(</span><span class="s1">'Component 2'</span><span class="p">);</span>
</code></pre></div></div> <hr/> <h2 id="limitations-and-challenges">Limitations and Challenges</h2> <h3 id="1-linear-assumption">1. Linear Assumption</h3> <p>SVD assumes linear relationships. It may miss:</p> <ul> <li>Nonlinear spectral mixing (intimate mixtures)</li> <li>Complex atmospheric effects</li> <li>Nonlinear sensor responses</li> </ul> <p><strong>Solution</strong>: Consider kernel PCA or manifold learning for nonlinear patterns.</p> <h3 id="2-global-analysis">2. Global Analysis</h3> <p>SVD finds global patterns but may miss:</p> <ul> <li>Localized anomalies</li> <li>Rare classes with unique spectra</li> <li>Spatial discontinuities</li> </ul> <p><strong>Solution</strong>: Apply SVD locally (tile-based) or use sparse methods.</p> <h3 id="3-noise-sensitivity">3. Noise Sensitivity</h3> <p>While SVD can denoise by removing small singular values, it can also:</p> <ul> <li>Spread systematic noise across components</li> <li>Fail to separate signal from structured noise</li> </ul> <p><strong>Solution</strong>: Preprocess with noise-adjusted methods (MNF, Minimum Noise Fraction).</p> <h3 id="4-interpretability">4. Interpretability</h3> <p>While mathematically optimal, SVD components are abstract:</p> <ul> <li>Not physically meaningful</li> <li>Mixed spectral-spatial patterns</li> <li>Difficult to relate to domain knowledge</li> </ul> <p><strong>Solution</strong>: Compare with domain-specific features (vegetation indices, mineral indices).</p> <hr/> <h2 id="future-extensions-and-research-directions">Future Extensions and Research Directions</h2> <h3 id="1-temporal-analysis">1. Temporal Analysis</h3> <p>For time-series hyperspectral data:</p> <ul> <li>Apply SVD to spatiotemporal cube</li> <li>Track component changes over time</li> <li>Detect phenological changes</li> </ul> <h3 id="2-fusion-with-other-data">2. Fusion with Other Data</h3> <p>Combine hyperspectral with:</p> <ul> <li>LiDAR (elevation data)</li> <li>SAR (radar data)</li> <li>Multispectral (higher spatial resolution)</li> </ul> <p>Use joint SVD or coupled matrix factorization.</p> <h3 id="3-deep-learning-integration">3. Deep Learning Integration</h3> <ul> <li>Use SVD for pre-training autoencoders</li> <li>Initialize neural network layers with SVD components</li> <li>Combine SVD with CNN for hybrid models</li> </ul> <h3 id="4-real-time-processing">4. Real-Time Processing</h3> <ul> <li>Implement online/incremental SVD</li> <li>Parallel processing on GPU</li> <li>Edge computing for drone/satellite platforms</li> </ul> <h3 id="5-uncertainty-quantification">5. Uncertainty Quantification</h3> <ul> <li>Bootstrap SVD for confidence intervals</li> <li>Bayesian approaches to SVD</li> <li>Propagate uncertainty through reconstruction</li> </ul> <hr/> <h2 id="conclusions">Conclusions</h2> <p>This comprehensive study demonstrates the power and elegance of Singular Value Decomposition for hyperspectral image analysis. Key takeaways:</p> <h3 id="theoretical-insights">Theoretical Insights</h3> <ul> <li>SVD provides the mathematically optimal low-rank approximation</li> <li>Singular values reveal the intrinsic dimensionality of data</li> <li>The method is interpretable, stable, and well-understood</li> </ul> <h3 id="practical-results">Practical Results</h3> <ul> <li>Achieved 96.35% dimensionality reduction while retaining 99% of variance</li> <li>Only 2 components needed for the Indian Pines dataset</li> <li>Reconstruction error &lt;1%, visually imperceptible</li> <li>Compression ratio of 108:1</li> </ul> <h3 id="applications">Applications</h3> <ul> <li>Efficient storage and transmission</li> <li>Preprocessing for machine learning</li> <li>Data visualization and exploration</li> <li>Noise reduction and quality enhancement</li> </ul> <h3 id="best-practices">Best Practices</h3> <ol> <li>Always visualize singular value decay</li> <li>Use cumulative variance to select rank</li> <li>Validate reconstruction quality visually and numerically</li> <li>Consider domain knowledge when interpreting components</li> <li>Compare with alternative methods for your specific application</li> </ol> <p>SVD remains a fundamental tool in the hyperspectral imaging pipeline, balancing mathematical rigor with computational efficiency and practical utility.</p> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Jolliffe, I. T., &amp; Cadima, J. (2016)</strong><br/> “Principal component analysis: a review and recent developments”<br/> <em>Philosophical Transactions of the Royal Society A</em>, 374(2065), 20150202.</p> </li> <li> <p><strong>Green, A. A., et al. (1988)</strong><br/> “A transformation for ordering multispectral data in terms of image quality with implications for noise removal”<br/> <em>IEEE Transactions on Geoscience and Remote Sensing</em>, 26(1), 65-74. [Introduces Minimum Noise Fraction (MNF)]</p> </li> <li> <p><strong>Trefethen, L. N., &amp; Bau III, D. (1997)</strong><br/> “Numerical Linear Algebra”<br/> <em>SIAM</em>, Philadelphia. [Comprehensive treatment of SVD and numerical methods]</p> </li> <li> <p><strong>Golub, G. H., &amp; Van Loan, C. F. (2013)</strong><br/> “Matrix Computations” (4th ed.)<br/> <em>Johns Hopkins University Press</em>. [Standard reference for SVD algorithms]</p> </li> <li> <p><strong>Landgrebe, D. (2003)</strong><br/> “Signal Theory Methods in Multispectral Remote Sensing”<br/> <em>Wiley-Interscience</em>. [Hyperspectral image analysis fundamentals]</p> </li> </ol> <hr/> <h2 id="code-and-data-availability">Code and Data Availability</h2> <p><strong>Dataset</strong>: Indian Pines hyperspectral image</p> <ul> <li>Source: Purdue University</li> <li>Format: MATLAB <code class="language-plaintext highlighter-rouge">.mat</code> file</li> <li>Size: 145×145×220 (4.6M elements)</li> </ul> <p><strong>Code</strong>: Complete MATLAB implementation available at [GitHub repository link]</p> <p><strong>Dependencies</strong>:</p> <ul> <li>MATLAB R2018b or later</li> <li>Image Processing Toolbox (for visualization functions)</li> <li>Statistics and Machine Learning Toolbox (optional, for additional analysis)</li> </ul> <hr/> <p><em>For questions, suggestions, or collaborations, please open an issue on GitHub or contact via email.</em></p> <p><strong>Last updated</strong>: January 2025</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="linear-algebra,"/><category term="svd,"/><category term="hyperspectral,dimensionality-reduction"/><summary type="html"><![CDATA[Understanding SVD theory and applying it to hyperspectral imaging - A comprehensive guide]]></summary></entry><entry><title type="html">Logistic Regression for Ultrasonic Flow Meter Health Classification</title><link href="https://olivierkanamugire.github.io/blog/2025/logistic_regression/" rel="alternate" type="text/html" title="Logistic Regression for Ultrasonic Flow Meter Health Classification"/><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/logistic_regression</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/logistic_regression/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Condition monitoring and diagnostics play a critical role in ensuring the reliability of industrial measurement systems. In ultrasonic flow metering, installation effects can significantly distort measurements, leading to inaccurate flow estimation and potential operational risks.</p> <p>This work focuses on developing a <strong>binary classification model based on logistic regression</strong> to determine whether an 8-path liquid ultrasonic flow meter (USM) is operating in a <strong>healthy state</strong> or is affected by <strong>installation effects</strong>. Using diagnostic parameters derived from experimental observations, we formulate the problem as a supervised learning task and analyze both optimization behavior and classification performance.</p> <hr/> <h2 id="problem-formulation">Problem Formulation</h2> <p>Given a dataset<br/> \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^{N},\) where $x_i \in \mathbb{R}^{36}$ represents diagnostic features and</p> \[y_i \in \{0,1\}\] <p>denotes the health state of the flow meter, the objective is to learn a parameter vector $\beta$ that models the probability</p> \[P(y = 1 \mid x) = \sigma(x^\top \beta),\] <p>where $\sigma(\cdot)$ is the sigmoid function.</p> <hr/> <h2 id="about-the-dataset">About the Dataset</h2> <p>The dataset, referred to as <strong>Meter A</strong>, consists of 87 experimental observations collected from an 8-path liquid ultrasonic flow meter .</p> <p><strong>Key characteristics:</strong></p> <ul> <li>87 total samples</li> <li>36 continuous input features</li> <li>Binary target variable: <ul> <li><code class="language-plaintext highlighter-rouge">1</code> → Healthy condition</li> <li><code class="language-plaintext highlighter-rouge">0</code> → Installation effect</li> </ul> </li> </ul> <p>The relatively small dataset size motivated careful preprocessing and evaluation to avoid biased conclusions.</p> <hr/> <h2 id="data-preprocessing">Data Preprocessing</h2> <h3 id="feature-normalization">Feature Normalization</h3> <p>Initial exploration revealed that the predictors varied significantly in scale and dispersion. To ensure numerical stability during optimization, all features were standardized using <strong>Z-score normalization</strong>:</p> \[z = \frac{x - \mu}{\sigma},\] <p>where $\mu$ and $\sigma$ denote the mean and standard deviation of each feature.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/box1-480.webp 480w,/assets/img/ADML/box1-800.webp 800w,/assets/img/ADML/box1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/box1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/box2-480.webp 480w,/assets/img/ADML/box2-800.webp 800w,/assets/img/ADML/box2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/box2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>No missing values or categorical variables were detected in the dataset.</p> <hr/> <h2 id="traintest-splitting-strategy">Train–Test Splitting Strategy</h2> <p>Due to the sequential ordering of healthy and faulty observations, the dataset was <strong>shuffled prior to splitting</strong> to ensure representative samples in both subsets.</p> <p>Multiple split ratios were evaluated: 60:40, 75:25, 80:20.</p> <p>This allowed us to analyze how training data size affects convergence, generalization, and classification accuracy.</p> <hr/> <h2 id="logistic-regression-model">Logistic Regression Model</h2> <h3 id="sigmoid-function">Sigmoid Function</h3> <p>Logistic regression maps linear combinations of features to probabilities using the sigmoid function:</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}.\] <p>This formulation enables direct probabilistic interpretation of model outputs.</p> <hr/> <h2 id="cost-function-and-optimization">Cost Function and Optimization</h2> <p>The model parameters were estimated by minimizing the <strong>negative log-likelihood</strong>:</p> \[\mathcal{L}(\beta) = - \sum_{i=1}^{n} \left[ y_i \log(\sigma(X_i \beta)) + (1 - y_i)\log(1 - \sigma(X_i \beta)) \right].\] <h3 id="optimization-method">Optimization Method</h3> <ul> <li>Initial weights: <strong>zero initialization</strong></li> <li>Bias term embedded directly into the feature matrix</li> <li>Optimization performed using <strong>MATLAB’s <code class="language-plaintext highlighter-rouge">fminsearch</code></strong></li> <li>No explicit stopping criteria imposed</li> </ul> <p>A custom output function tracked:</p> <ul> <li>loss value per iteration</li> <li>total number of iterations until convergence</li> </ul> <p>This setup allowed us to study the <strong>natural convergence behavior</strong> of the optimizer.</p> <hr/> <h2 id="optimization-behavior">Optimization Behavior</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch60-480.webp 480w,/assets/img/ADML/fminsearch60-800.webp 800w,/assets/img/ADML/fminsearch60-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch60.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch75-480.webp 480w,/assets/img/ADML/fminsearch75-800.webp 800w,/assets/img/ADML/fminsearch75-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch75.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/fminsearch80-480.webp 480w,/assets/img/ADML/fminsearch80-800.webp 800w,/assets/img/ADML/fminsearch80-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/fminsearch80.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Key observations:</strong></p> <ul> <li>All splits show monotonic decrease in loss</li> <li>Larger training sets require more optimization iterations</li> <li>Absence of stopping criteria increases computation cost for larger datasets</li> </ul> <hr/> <h2 id="model-evaluation-metrics">Model Evaluation Metrics</h2> <p>Model performance was assessed using:</p> <ul> <li><strong>Accuracy</strong></li> <li><strong>Confusion Matrix</strong></li> <li><strong>Precision</strong></li> <li><strong>Recall</strong></li> </ul> \[\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}\] <hr/> <h2 id="classification-accuracy-results">Classification Accuracy Results</h2> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">pretty_table</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| split ratio | Tr. accuracy | Te. accuuracy |
| :---------- | :----------: | ------------: |
| 60:40       |    80.86     |         82.86 |
| 75:25       |    83.08     |        100.00 |
| 80:20       |     90.0     |         76.47 |
</code></pre></div></div> <p>The <strong>75:25 split</strong> achieved perfect test accuracy, though this should be interpreted cautiously due to the small sample size.</p> <hr/> <h2 id="feature-importance-analysis">Feature Importance Analysis</h2> <p>Across all splits, the <strong>25th feature</strong> (gain at the fifth end of each of the eight paths) consistently exhibited the largest coefficient magnitude.</p> <ul> <li>Strong influence regardless of split ratio</li> <li>Indicates a dominant diagnostic role in identifying installation effects</li> </ul> <hr/> <h2 id="confusion-matrix-analysis">Confusion Matrix Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf60-480.webp 480w,/assets/img/ADML/conf60-800.webp 800w,/assets/img/ADML/conf60-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf60.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf75-480.webp 480w,/assets/img/ADML/conf75-800.webp 800w,/assets/img/ADML/conf75-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf75.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ADML/conf80-480.webp 480w,/assets/img/ADML/conf80-800.webp 800w,/assets/img/ADML/conf80-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ADML/conf80.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="discussion">Discussion</h2> <p>This study demonstrates that logistic regression, when combined with appropriate normalization and numerical optimization, can effectively classify ultrasonic flow meter health states—even with limited data.</p> <p>However:</p> <ul> <li>Small datasets increase variance in performance metrics</li> <li>Absence of stopping criteria impacts computational efficiency</li> <li>Feature dominance suggests opportunities for dimensionality reduction</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Logistic regression remains a powerful and interpretable baseline for industrial diagnostic problems. Beyond classification accuracy, analyzing <strong>optimization dynamics</strong>, <strong>feature influence</strong>, and <strong>data splitting strategies</strong> provides deeper insight into model behavior and reliability.</p> <p>Future work may include:</p> <ul> <li>regularization</li> <li>alternative optimizers</li> <li>cross-validation</li> <li>nonlinear extensions</li> </ul> <hr/> <p><em>Code and experiments related to this project are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="machine-learning"/><category term="classification"/><category term="optimization"/><summary type="html"><![CDATA[Binary classification of ultrasonic flow meter health using logistic regression and numerical optimization]]></summary></entry><entry><title type="html">On the optimization algorithms and learning rate</title><link href="https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm/" rel="alternate" type="text/html" title="On the optimization algorithms and learning rate"/><published>2025-01-05T00:00:00+00:00</published><updated>2025-01-05T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2025/optimizationAlgorithm/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Optimization lies at the heart of machine learning. Nearly every learning problem can be framed as the task of minimizing (or maximizing) an objective function that quantifies error, loss, or energy. Whether we are fitting a linear model or training a deep neural network with millions of parameters, the learning process reduces to an optimization problem.</p> <p>This article introduces the fundamental ideas behind optimization in machine learning and gradually builds intuition for some of the most widely used algorithms: <strong>Gradient Descent (GD)</strong>, <strong>Stochastic Gradient Descent (SGD)</strong>, and <strong>Adam</strong>.</p> <hr/> <h2 id="optimization-as-a-minimization-problem">Optimization as a Minimization Problem</h2> <p>In supervised learning, we are given a dataset $\mathcal{D} = {(x_i, y_i)}_{i=1}^N$and a model parameterized by $\theta$. Learning consists of minimizing a loss function:</p> \[\theta^* = \arg\min_{\theta} \mathcal{L}(\theta), \quad \text{where} \quad \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell\big(f(x_i; \theta), y_i\big).\] <p>The loss surface can be convex or non-convex, smooth or highly irregular, low- or high-dimensional.</p> <p>Optimization algorithms define <em>how</em> we move on this surface to reach a good solution.</p> <hr/> <h2 id="gradient-descent-gd">Gradient Descent (GD)</h2> <p>Gradient Descent is the most fundamental first-order optimization method. It iteratively updates parameters in the direction of the negative gradient of the loss function:</p> \[\theta_{t+1} = \theta_t - \eta \nabla_{\theta}\mathcal{L}(\theta_t)\] <p>where $\nabla_{\theta}\mathcal{L}$ is the gradient and $\eta &gt; 0$ is the learning rate.</p> <h2 id="example-of-its-convergence">Example of its convergence</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/gradient_descent_convergence.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="intuition">Intuition</h3> <p>The gradient points in the direction of steepest ascent. Moving in the opposite direction ensures we descend toward a minimum. However, the choice of learning rate is critical.</p> <p>The animations here illustrate this clearly:</p> <ul> <li><strong>Small learning rate</strong>: stable but slow convergence</li> <li><strong>Large learning rate</strong>: faster movement but risk of overshooting or divergence</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/small_lr.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/large_lr.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <h3 id="limitations">Limitations</h3> <ul> <li>Requires computing gradients over the <em>entire dataset</em></li> <li>Slow for large-scale problems</li> <li>Sensitive to learning rate selection</li> </ul> <hr/> <h2 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h2> <p>To address scalability, <strong>Stochastic Gradient Descent</strong> approximates the full gradient using a single sample or a mini-batch:</p> \[\theta_{t+1} = \theta_t - \eta \nabla_{\theta}\ell(f(x_i; \theta_t), y_i)\] <h3 id="why-sgd-works">Why SGD Works</h3> <p>Although SGD introduces noise into the optimization process, this randomness often helps:</p> <ul> <li>escape shallow local minima</li> <li>improve generalization</li> <li>speed up training dramatically</li> </ul> <h3 id="trade-offs">Trade-offs</h3> <ul> <li>Faster per-iteration updates</li> <li>Noisy convergence</li> <li>Requires careful tuning of learning rate schedules</li> </ul> <hr/> <h2 id="momentum-based-methods">Momentum-Based Methods</h2> <p>Pure SGD may oscillate heavily, especially in ravines where curvature differs across dimensions. Momentum methods address this by accumulating past gradients:</p> <p>$v_t = \beta v_{t-1} + (1 - \beta)\nabla_{\theta}\mathcal{L}(\theta_t)$ $\theta_{t+1} = \theta_t - \eta v_t$</p> <p>Momentum smooths updates and accelerates convergence along consistent directions.</p> <hr/> <h2 id="adam-optimizer">Adam Optimizer</h2> <p><strong>Adam (Adaptive Moment Estimation)</strong> combines:</p> <ul> <li>momentum (first moment)</li> <li>adaptive learning rates (second moment)</li> </ul> <p>It maintains running estimates of:</p> <ul> <li>mean of gradients</li> <li>uncentered variance of gradients</li> </ul> <p>Update rules:</p> <p>\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\) \(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\)</p> <p>After bias correction:</p> \[\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\] <h2 id="example-of-its-convergence-1">Example of its convergence</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/adam_convergence.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="why-adam-is-popular">Why Adam is Popular</h3> <ul> <li>Minimal tuning</li> <li>Handles sparse gradients well</li> <li>Works robustly in deep learning</li> </ul> <p>However, Adam is not always optimal for generalization and may converge to worse minima than SGD in some settings.</p> <hr/> <h2 id="choosing-the-right-optimizer">Choosing the Right Optimizer</h2> <p>There is no universally optimal optimizer. The choice depends on:</p> <ul> <li>dataset size</li> <li>model complexity</li> <li>smoothness of the loss</li> <li>generalization requirements</li> </ul> <p><strong>Rule of thumb</strong>:</p> <ul> <li>Start with <strong>Adam</strong> for fast prototyping</li> <li>Switch to <strong>SGD + momentum</strong> for better generalization when training stabilizes</li> </ul> <hr/> <h2 id="final-remarks">Final Remarks</h2> <p>Optimization algorithms define the dynamics of learning. Understanding how and why they work is essential not only for training models efficiently, but also for diagnosing failure modes such as divergence, slow convergence, or poor generalization.</p> <p>From classical Gradient Descent to adaptive methods like Adam, optimization remains an active and evolving research area—especially in large-scale and multimodal learning systems.</p> <hr/> <p><em>Code implementations and experiments related to these algorithms are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[Gradient descent (GD), Stochastic GD, Adam optimizer]]></summary></entry><entry><title type="html">Graphs and Graph Neural Nets</title><link href="https://olivierkanamugire.github.io/blog/2024/graphNN/" rel="alternate" type="text/html" title="Graphs and Graph Neural Nets"/><published>2024-12-20T00:00:00+00:00</published><updated>2024-12-20T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2024/graphNN</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2024/graphNN/"><![CDATA[<h2 id="graph-neural-nets-and-graph-convolutional-nets">Graph Neural Nets and Graph Convolutional Nets</h2> <p>Graphs are very rich mathematical-defined data structures. Research on analysing graphs with machine learning has been receiving more and more attention because of the great expressive power of graphs. Some real-world problems do not fit into sequential data-related or grid structures like images. Due to graph structure, it enables relational reasoning and efficient representation learning for entities connected in a network. They excel in problems involving relationships between entities. Another thing is that it captures a variety of settings: supervised, semi-supervised and unsupervised.</p> <hr/> <h2 id="problems-that-gnns-and-gcns-are-particularly-good-at-addressing">Problems That GNNs and GCNs Are Particularly Good at Addressing:</h2> <p><strong>Node Classification</strong> – Determining the labels of nodes by analyzing their features and neighbouring nodes (e.g., identifying fraud in financial transactions).</p> <p><strong>Link Prediction</strong> – Anticipating absent or upcoming connections among nodes (e.g., suggesting friends in social networks).</p> <p><strong>Graph Classification</strong> – Categorizing whole graphs (e.g., assessing whether a chemical compound is harmful or safe).</p> <p><strong>Clustering and Community Detection</strong> – Recognizing clusters within networks (e.g., identifying similar consumers in e-commerce).</p> <p><strong>Knowledge Graph Completion</strong> – Deducing absent relationships between entities within a knowledge base (e.g., systems for answering questions).</p> <h2 id="applications">Applications</h2> <p><strong>Biomedical Engineering.</strong> With the Protein-Protein Interaction Network, graph convolution and relation network for breast cancer subtype classification can be used and provide efficient results. In addition, a GCN-based model for polypharmacy side effects prediction. They can be used to model the drug and protein interaction network and separately deal with edges in different types.</p> <p><strong>Combinatorial optimization.</strong> Combinatorial optimization issues related to graphs comprise a collection of NP-hard challenges that draw significant interest from researchers across various disciplines. Certain well-known problems, such as the travelling salesman problem (TSP) and minimum spanning trees (MST), have seen numerous heuristic approaches developed. Lately, employing deep neural networks to tackle these problems has gained popularity, with some solutions additionally utilizing graph neural networks due to their inherent graph characteristics.</p> <p><strong>Traffic networks.</strong> Forecasting traffic conditions is a difficult undertaking due to the dynamic nature of traffic networks and their intricate dependencies. Certain studies have integrated GNNs with LSTMs to effectively model both spatial and temporal relationships. Moreover, the use of ST-Conv blocks, which incorporate spatial and temporal convolution layers along with residual connections and bottleneck techniques, has demonstrated excellent outcomes.</p> <p>Other ones are the distribution of water or electricity as they require the shortest distance problem and graph framework poses as a great tool for solution.</p> <h3 id="adjacency-matrix">Adjacency matrix</h3> \[A = \begin{array}{c|cccccccc} &amp; N_1 &amp; N_2 &amp; N_3 &amp; N_4 &amp; N_5 &amp; N_6 &amp; N_7 &amp; N_8 \\ \hline N_1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ N_2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ N_3 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\ N_4 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ N_5 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ N_6 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ N_7 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ N_8 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \end{array}\] <h3 id="degree-matrix">Degree matrix</h3> \[D = \begin{pmatrix} 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}\] <h3 id="laplacian-matrix">Laplacian matrix</h3> \[L = \begin{array}{c|cccccccc} &amp; N_1 &amp; N_2 &amp; N_3 &amp; N_4 &amp; N_5 &amp; N_6 &amp; N_7 &amp; N_8 \\ \hline N_1 &amp; 2 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ N_2 &amp; -1 &amp; 3 &amp; 0 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\ N_3 &amp; -1 &amp; 0 &amp; 3 &amp; 0 &amp; 0 &amp; -1 &amp; -1 &amp; 0 \\ N_4 &amp; 0 &amp; -1 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; -1 \\ N_5 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; -1 \\ N_6 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 2 &amp; -1 &amp; 0 \\ N_7 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; -1 &amp; 2 &amp; 0 \\ N_8 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 2 \end{array}\] <p>The second part of the code implements a diffusion process on a graph using a symmetrically normalized adjacency matrix. This process simulates how information propagates across the nodes of the graph over multiple iterations.</p> <h2 id="step-by-step-explanation">Step-by-step explanation</h2> <h3 id="modified-adjacency-matrix-a">Modified adjacency matrix $A^*$</h3> <p>The adjacency matrix $A$ represents the structure of the graph, where</p> \[A_{ij} = \begin{cases} 1, &amp; \text{if there is an edge between nodes } i \text{ and } j, \\ 0, &amp; \text{otherwise}. \end{cases}\] <p>To ensure that each node preserves part of its own information during propagation, self-loops are added to the graph. This leads to the modified adjacency matrix \(A^* = A + I\) where $I$ denotes the identity matrix.</p> <h2 id="degree-matrix-d">Degree matrix $D^*$</h2> <p>The degree matrix (D^*) is a diagonal matrix whose entries correspond to the sum of each row of (A^*), i.e.,</p> \[D^*_{ii} = \sum_{j} A^\*\_{ij}.\] <h2 id="symmetrically-normalized-adjacency-matrix-tildea">Symmetrically normalized adjacency matrix $\tilde{A}$</h2> <p>Rather than using the modified adjacency matrix directly, a symmetric normalization is applied, inspired by spectral graph theory:</p> \[\tilde{A} = D^{\*-\frac{1}{2}} A^\* D^{\*-\frac{1}{2}}.\] <p>This normalization balances the influence of nodes with different degrees and ensures numerical stability during diffusion.</p> <h2 id="propagation-of-information">Propagation of information</h2> <p>A diffusion (or message-passing) process is simulated over the graph. At each iteration, node representations are updated according to \(H^{(t+1)} = \tilde{A} H^{(t)},\) where $H^{(t)}$ denotes the node information at iteration $t$. The representations at each iteration are stored for later analysis and visualization.</p> <h2 id="visualization">Visualization</h2> <p>The diffusion process is visualized using <strong>networkx</strong> for graph construction and <strong>matplotlib.animation</strong> for animation. Nodes are colored according to their information values, illustrating how the initial signal at node 0 spreads through the network over time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/graphs.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p><em>Code implementations and experiments related to these algorithms are available in the accompanying repository.</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[Graphs, Adjacency Matrix, Graph Neural Nets]]></summary></entry><entry><title type="html">K-Nearest Neighbor (KNN) – How your closest connections define you!</title><link href="https://olivierkanamugire.github.io/blog/2024/knn/" rel="alternate" type="text/html" title="K-Nearest Neighbor (KNN) – How your closest connections define you!"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://olivierkanamugire.github.io/blog/2024/knn</id><content type="html" xml:base="https://olivierkanamugire.github.io/blog/2024/knn/"><![CDATA[<p>K-Nearest Neighbor (KNN) is an intuitive, supervised machine learning algorithm named a lazy learner. That is because rather than constructing an explicit training model, KNN makes predictions by directly relying on the surrounding data points—its <code class="language-plaintext highlighter-rouge">neighbors</code>—to classify or determine the label of a new instance. This behavior can be analogized to human interactions: to understand someone, we often observe their social circle, as their friends tend to influence or describe who they are. In the context of KNN, the challenge lies in determining the optimal number of neighbors (k) to consider in order to best describe or classify a new point.</p> <p>From a mathematical perspective, KNN determines “closeness” by calculating <code class="language-plaintext highlighter-rouge">distances</code> between data points. Machines operate on numerical input, so distance metrics serve as a foundation for determining which neighbors are closest. Various distance measures exist, such as the Euclidean distance (L2 norm), Manhattan distance (L1 norm), and others. Selecting the appropriate distance metric is critical, as it influences the algorithm’s performance and suitability for specific problems.</p> <p>In this work, we provide a complete implementation of the KNN algorithm from scratch. We define custom functions to compute the necessary distances, identify neighbors, and make predictions, offering a comprehensive understanding of the model’s inner workings.</p> <h2 id="custom-sum-function">custom sum function</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fixed_custom_sum</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># Sum of all elements
</span>        <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">row</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Column-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Row-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid axis. Use 0 for columns or 1 for rows.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="sorting">sorting</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

</code></pre></div></div> <h2 id="distance">distance</h2> <p>Now we define euclidean distance we will utilize. \(d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\)</p> <p>This is very nice distance computing because it is implemented as vector computation. Therefore,By avoiding loops and using vectorized operations for distance calculation, we significantly improved the performance of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
    <span class="n">num_train_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span>
    <span class="n">num_test_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">testdata</span><span class="p">)</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_test_samples</span><span class="p">):</span>
        <span class="n">test_sample</span> <span class="o">=</span> <span class="n">testdata</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">test_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train_samples</span><span class="p">):</span>
            <span class="n">train_sample</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">squared_diff</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">train_sample</span><span class="p">,</span> <span class="n">test_sample</span><span class="p">)]</span>
            <span class="n">test_distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">custom_sum</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
        <span class="n">distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">test_distances</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distances</span>
</code></pre></div></div> <h2 id="majority-vote-for-classification">Majority vote for classification</h2> <p>Here, we choose the class that represents the majority of the closest neighbors, because the labels of the nearest neighbors collectively determine the final prediction for the test point.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">majority_vote</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">label_count</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Find the label with the maximum count
</span>    <span class="n">max_count</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">most_common_label</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">max_count</span><span class="p">:</span>
            <span class="n">max_count</span> <span class="o">=</span> <span class="n">count</span>
            <span class="n">most_common_label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">return</span> <span class="n">most_common_label</span>
</code></pre></div></div> <h2 id="knn-implementation-and-its-instantiation">KNN implementation and its instantiation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">traindata</span> <span class="o">=</span> <span class="n">traindata</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trainclass</span> <span class="o">=</span> <span class="n">trainclass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">)</span>

        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">distances</span><span class="p">:</span>
            <span class="n">k_indices</span> <span class="o">=</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">d</span><span class="p">)[:</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">]</span>
            <span class="n">k_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">trainclass</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
            <span class="n">k_nearest_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">majority_vote</span><span class="p">(</span><span class="n">k_labels</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">k_nearest_labels</span>
</code></pre></div></div> <p>Contribution</p> <p>No Libraries for Core Algorithm: This project is a pure implementation of KNN, providing a deeper understanding of how the algorithm works under the hood.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[KNN algorithm from scratch]]></summary></entry></feed>